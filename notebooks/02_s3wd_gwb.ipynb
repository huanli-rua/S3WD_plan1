{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 · S3WD v02 主线：参考元组 + 相似度 + 漂移闭环\n",
    "\n",
    "本 Notebook 演示 S3WD v02 流程：利用 Reference Tuple（参考元组）/混合相似度/批级阈值小网格，结合漂移分级响应（S1/S2/S3）。\n",
    "运行前请确认 `configs/s3wd_airline.yaml` 已更新至 v02 键位，并准备好 Airlines 数据。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='[%(levelname)s] %(message)s')\n",
    "\n",
    "print(\"【步骤0】环境信息：\", {\n",
    "    'python': sys.version.split()[0],\n",
    "    'cwd': os.getcwd(),\n",
    "    'project_root': str(Path.cwd())\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3wdlib.config_loader import load_yaml_cfg, extract_vars, show_cfg\n",
    "from s3wdlib import (\n",
    "    augment_airline_features,\n",
    "    assign_buckets,\n",
    "    build_ref_tuples,\n",
    "    combine_history,\n",
    "    corr_to_set,\n",
    "    to_trisect_probs,\n",
    "    expected_cost,\n",
    "    expected_fbeta,\n",
    "    select_alpha_beta,\n",
    "    ema_clip,\n",
    "    detect_drift,\n",
    "    apply_actions,\n",
    "    compute_region_masks,\n",
    ")\n",
    "import s3wdlib.bucketizer as bucketizer\n",
    "import s3wdlib.similarity as similarity\n",
    "import s3wdlib.drift_controller as drift_controller\n",
    "from s3wdlib.gwb import GWBProbEstimator\n",
    "\n",
    "CONFIG_PATH = Path('../configs/s3wd_airline.yaml')\n",
    "cfg = load_yaml_cfg(str(CONFIG_PATH))\n",
    "show_cfg(cfg)\n",
    "vars_flat = extract_vars(cfg)\n",
    "\n",
    "bucketizer.configure(cfg.get('BUCKET'))\n",
    "similarity.configure(cfg.get('SIMILARITY'))\n",
    "\n",
    "sim_cfg = similarity.current_config()\n",
    "drift_thresholds = {\n",
    "    'psi_thresholds': cfg['DRIFT'].get('psi_thresholds'),\n",
    "    'tv_thresholds': cfg['DRIFT'].get('tv_thresholds'),\n",
    "    'posrate_shift': cfg['DRIFT'].get('posrate_shift'),\n",
    "    'perf_drop': cfg['DRIFT'].get('perf_drop'),\n",
    "    'debounce_windows': cfg['DRIFT'].get('debounce_windows', 1),\n",
    "}\n",
    "actions_cfg = cfg['DRIFT'].get('actions', {})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from s3wdlib.data_io import load_table_auto\n",
    "\n",
    "data_dir = Path(cfg['DATA']['data_dir'])\n",
    "data_path = data_dir / cfg['DATA']['data_file']\n",
    "X_raw, y = load_table_auto(\n",
    "    str(data_path),\n",
    "    continuous_label=cfg['DATA'].get('continuous_label'),\n",
    "    threshold=cfg['DATA'].get('threshold'),\n",
    "    threshold_op=cfg['DATA'].get('threshold_op')\n",
    ")\n",
    "X_enriched = augment_airline_features(X_raw)\n",
    "if 'Month' in X_enriched.columns:\n",
    "    X_enriched['Month'] = X_enriched['Month'].astype(int)\n",
    "\n",
    "months = sorted(X_enriched['Month'].unique().tolist())\n",
    "warmup_months = months[:6] if len(months) >= 6 else months[:-1]\n",
    "stream_months = [m for m in months if m not in warmup_months]\n",
    "train_mask = X_enriched['Month'].isin(warmup_months)\n",
    "X_train = X_enriched.loc[train_mask].reset_index(drop=True)\n",
    "y_train = y.loc[train_mask].reset_index(drop=True)\n",
    "X_stream = X_enriched.loc[~train_mask].reset_index(drop=True)\n",
    "y_stream = y.loc[~train_mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"【数据划分】训练窗={warmup_months}，流式窗={stream_months}，训练样本={len(X_train)}，流式样本={len(X_stream)}\")\n",
    "\n",
    "bucket_train = assign_buckets(X_train)\n",
    "cat_cols = [c for c in ['UniqueCarrier','Origin','Dest','DayOfWeek','Month'] if c in X_train.columns]\n",
    "\n",
    "gwb_cfg = cfg['GWB']\n",
    "gwb_estimator = GWBProbEstimator(\n",
    "    k=gwb_cfg.get('k', 6),\n",
    "    metric=gwb_cfg.get('metric', 'euclidean'),\n",
    "    eps=gwb_cfg.get('eps', 1e-6),\n",
    "    mode=gwb_cfg.get('mode', 'epanechnikov'),\n",
    "    bandwidth=gwb_cfg.get('bandwidth'),\n",
    "    bandwidth_scale=gwb_cfg.get('bandwidth_scale', 1.0),\n",
    "    use_faiss=gwb_cfg.get('use_faiss', True),\n",
    "    faiss_gpu=gwb_cfg.get('faiss_gpu', True),\n",
    "    categorical_features=cat_cols,\n",
    "    category_penalty=gwb_cfg.get('category_penalty', 0.5),\n",
    ")\n",
    "gwb_estimator.fit(\n",
    "    X_train.values,\n",
    "    y_train.to_numpy(),\n",
    "    categorical_values=X_train[cat_cols] if cat_cols else None,\n",
    ")\n",
    "\n",
    "if cfg['REF_TUPLE'].get('use_gwb_weight', True):\n",
    "    gwb_prob_train = gwb_estimator.predict_proba(\n",
    "        X_train.values,\n",
    "        categorical_values=X_train[cat_cols] if cat_cols else None,\n",
    "    )\n",
    "else:\n",
    "    gwb_prob_train = None\n",
    "\n",
    "ref_tuples = build_ref_tuples(\n",
    "    X_train,\n",
    "    y_train.to_numpy(),\n",
    "    bucket_train,\n",
    "    topk_per_class=cfg['REF_TUPLE'].get('topk_per_class', 256),\n",
    "    pos_quantile=cfg['REF_TUPLE'].get('pos_quantile', 0.7),\n",
    "    keep_history_ratio=cfg['REF_TUPLE'].get('keep_history_ratio', 0.3),\n",
    "    gwb_prob=gwb_prob_train,\n",
    ")\n",
    "print(f\"【训练资产】参考元组覆盖桶数量={len(ref_tuples)}\")\n",
    "\n",
    "def compute_similarity_block(X_block, bucket_ids, ref_map, sigma, sim_params):\n",
    "    cat_weights = sim_params.get('cat_weights', {})\n",
    "    combine = sim_params.get('combine', 'product')\n",
    "    mix_alpha = sim_params.get('mix_alpha', 0.7)\n",
    "    E_pos = np.zeros(len(X_block), dtype=float)\n",
    "    E_neg = np.zeros(len(X_block), dtype=float)\n",
    "    index_map = {}\n",
    "    for idx, bucket in enumerate(bucket_ids):\n",
    "        index_map.setdefault(str(bucket), []).append(idx)\n",
    "    for bucket_id, positions in index_map.items():\n",
    "        refs = ref_map.get(bucket_id)\n",
    "        if not refs:\n",
    "            continue\n",
    "        subset = X_block.iloc[positions]\n",
    "        if refs.get('pos'):\n",
    "            E_pos[positions] = corr_to_set(\n",
    "                subset, refs['pos'], sigma, cat_weights, combine=combine, mix_alpha=mix_alpha\n",
    "            )\n",
    "        if refs.get('neg'):\n",
    "            E_neg[positions] = corr_to_set(\n",
    "                subset, refs['neg'], sigma, cat_weights, combine=combine, mix_alpha=mix_alpha\n",
    "            )\n",
    "    return E_pos, E_neg\n",
    "\n",
    "def calc_distribution_metrics(base_probs, curr_probs, bins):\n",
    "    base_hist, _ = np.histogram(base_probs, bins=bins)\n",
    "    curr_hist, _ = np.histogram(curr_probs, bins=bins)\n",
    "    base_ratio = np.clip(base_hist / max(base_hist.sum(), 1), 1e-6, None)\n",
    "    curr_ratio = np.clip(curr_hist / max(curr_hist.sum(), 1), 1e-6, None)\n",
    "    psi = float(np.sum((curr_ratio - base_ratio) * np.log(curr_ratio / base_ratio)))\n",
    "    tv = float(0.5 * np.sum(np.abs(curr_ratio - base_ratio)))\n",
    "    return psi, tv\n",
    "\n",
    "E_pos_train, E_neg_train = compute_similarity_block(\n",
    "    X_train, bucket_train, ref_tuples, sigma=sim_cfg.get('sigma', 0.5), sim_params=sim_cfg\n",
    ")\n",
    "P_train = to_trisect_probs(E_pos_train, E_neg_train)\n",
    "\n",
    "objective = cfg['MEASURE'].get('objective', 'expected_cost')\n",
    "measure_grid = cfg['MEASURE'].get('grid', {'alpha': [0.55, 0.9, 0.02], 'beta': [0.05, 0.45, 0.02]})\n",
    "measure_constraints = cfg['MEASURE'].get('constraints', {})\n",
    "measure_costs = cfg['MEASURE'].get('costs', {})\n",
    "beta_weight = cfg['MEASURE'].get('beta_weight', 1.0)\n",
    "\n",
    "alpha_prev, beta_prev, base_score = select_alpha_beta(\n",
    "    P_train, measure_grid, measure_constraints, objective=objective, costs=measure_costs, beta_weight=beta_weight\n",
    ")\n",
    "if objective == 'expected_cost':\n",
    "    baseline_perf = expected_cost(\n",
    "        P_train,\n",
    "        alpha_prev,\n",
    "        beta_prev,\n",
    "        measure_costs.get('c_fn', 1.0),\n",
    "        measure_costs.get('c_fp', 1.0),\n",
    "        measure_costs.get('c_bnd', 0.5),\n",
    "    )\n",
    "else:\n",
    "    baseline_perf = expected_fbeta(P_train, alpha_prev, beta_prev, beta_weight=beta_weight)\n",
    "\n",
    "baseline_bins = np.linspace(0, 1, 11)\n",
    "baseline_info = {\n",
    "    'p_pos': P_train['p_pos'],\n",
    "    'posrate': float(y_train.mean()),\n",
    "    'perf': baseline_perf,\n",
    "    'bins': baseline_bins,\n",
    "}\n",
    "\n",
    "runtime_state = {\n",
    "    'sigma': sim_cfg.get('sigma', 0.5),\n",
    "    'constraints': dict(measure_constraints),\n",
    "    'step_cap': dict(cfg['SMOOTH'].get('step_cap', {'alpha': 0.08, 'beta': 0.08})),\n",
    "    'actions_cfg': actions_cfg,\n",
    "    'keep_history_ratio': cfg['REF_TUPLE'].get('keep_history_ratio', 0.3),\n",
    "    'use_gwb_weight': cfg['REF_TUPLE'].get('use_gwb_weight', True),\n",
    "    'ref_tuples': ref_tuples,\n",
    "    'gwb_estimator': gwb_estimator,\n",
    "    'cat_weights': sim_cfg.get('cat_weights', {}),\n",
    "    'combine': sim_cfg.get('combine', 'product'),\n",
    "    'mix_alpha': sim_cfg.get('mix_alpha', 0.7),\n",
    "}\n",
    "history_records = [\n",
    "    {\n",
    "        'X': X_train.copy(),\n",
    "        'y': y_train.to_numpy(),\n",
    "        'buckets': bucket_train.copy(),\n",
    "        'gwb': gwb_prob_train if gwb_prob_train is not None else None,\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"【训练阈值】α0={alpha_prev:.3f}，β0={beta_prev:.3f}，基线性能={baseline_perf:.4f}\")\n",
    "print(f\"【状态初始化】sigma={runtime_state['sigma']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_trace = []\n",
    "window_metrics = []\n",
    "drift_events = []\n",
    "max_history_windows = 6\n",
    "metric_name = '预计成本' if objective == 'expected_cost' else '期望Fβ'\n",
    "\n",
    "if not stream_months:\n",
    "    print(\"【提示】当前数据不足以拆分流式窗口，请调整 warmup_months。\")\n",
    "\n",
    "for month in stream_months:\n",
    "    block_mask = X_enriched['Month'] == month\n",
    "    X_block = X_enriched.loc[block_mask].reset_index(drop=True)\n",
    "    y_block = y.loc[block_mask].reset_index(drop=True)\n",
    "    if X_block.empty:\n",
    "        continue\n",
    "\n",
    "    buckets_block = assign_buckets(X_block)\n",
    "    E_pos_block, E_neg_block = compute_similarity_block(\n",
    "        X_block,\n",
    "        buckets_block,\n",
    "        runtime_state['ref_tuples'],\n",
    "        sigma=runtime_state.get('sigma', 0.5),\n",
    "        sim_params=runtime_state,\n",
    "    )\n",
    "    P_block = to_trisect_probs(E_pos_block, E_neg_block)\n",
    "\n",
    "    alpha_star, beta_star, grid_score = select_alpha_beta(\n",
    "        P_block,\n",
    "        measure_grid,\n",
    "        runtime_state['constraints'],\n",
    "        objective=objective,\n",
    "        costs=measure_costs,\n",
    "        beta_weight=beta_weight,\n",
    "    )\n",
    "\n",
    "    step_caps = runtime_state.get('step_cap', {'alpha': 0.08, 'beta': 0.08})\n",
    "    alpha_smooth, beta_smooth = ema_clip(\n",
    "        alpha_star,\n",
    "        beta_star,\n",
    "        alpha_prev,\n",
    "        beta_prev,\n",
    "        cfg['SMOOTH'].get('ema_alpha', 0.6),\n",
    "        step_caps.get('alpha', 0.08),\n",
    "        step_caps.get('beta', 0.08),\n",
    "    )\n",
    "    if alpha_prev is None or beta_prev is None:\n",
    "        alpha_smooth, beta_smooth = alpha_star, beta_star\n",
    "\n",
    "    alpha_prev, beta_prev = alpha_smooth, beta_smooth\n",
    "\n",
    "    positive, negative, boundary = compute_region_masks(P_block, alpha_smooth, beta_smooth)\n",
    "    pos_cov = float(positive.mean())\n",
    "    bnd_ratio = float(boundary.mean())\n",
    "\n",
    "    if objective == 'expected_cost':\n",
    "        score_smoothed = expected_cost(\n",
    "            P_block,\n",
    "            alpha_smooth,\n",
    "            beta_smooth,\n",
    "            measure_costs.get('c_fn', 1.0),\n",
    "            measure_costs.get('c_fp', 1.0),\n",
    "            measure_costs.get('c_bnd', 0.5),\n",
    "        )\n",
    "        perf_drop = max(0.0, score_smoothed - baseline_info['perf'])\n",
    "    else:\n",
    "        score_smoothed = expected_fbeta(P_block, alpha_smooth, beta_smooth, beta_weight=beta_weight)\n",
    "        perf_drop = max(0.0, baseline_info['perf'] - score_smoothed)\n",
    "\n",
    "    psi_val, tv_val = calc_distribution_metrics(baseline_info['p_pos'], P_block['p_pos'], baseline_info['bins'])\n",
    "    posrate_curr = float(y_block.mean())\n",
    "    posrate_gap = abs(posrate_curr - baseline_info['posrate'])\n",
    "\n",
    "    stats_curr = {\n",
    "        'window_id': int(month),\n",
    "        'psi': psi_val,\n",
    "        'tv': tv_val,\n",
    "        'posrate': posrate_gap,\n",
    "        'perf_drop': perf_drop,\n",
    "    }\n",
    "    sigma_before = runtime_state.get('sigma', 0.5)\n",
    "    drift_level = detect_drift(stats_curr, baseline_info, drift_thresholds)\n",
    "    runtime_state = apply_actions(drift_level, runtime_state)\n",
    "\n",
    "    rebuild_tuple_flag = runtime_state.get('rebuild_ref_tuple', False)\n",
    "    rebuild_gwb_flag = runtime_state.get('rebuild_gwb_index', False)\n",
    "    redo_flag = runtime_state.get('redo_threshold', False)\n",
    "\n",
    "    actions_taken = []\n",
    "    if drift_level != 'NONE':\n",
    "        if runtime_state.get('sigma', sigma_before) != sigma_before:\n",
    "            actions_taken.append(f\"调σ→{runtime_state['sigma']:.3f}\")\n",
    "        if rebuild_tuple_flag:\n",
    "            actions_taken.append('重建参考元组')\n",
    "        if rebuild_gwb_flag:\n",
    "            actions_taken.append('重建GWB索引')\n",
    "        if redo_flag:\n",
    "            actions_taken.append('阈值重置')\n",
    "\n",
    "    if redo_flag:\n",
    "        alpha_prev, beta_prev = None, None\n",
    "        runtime_state['redo_threshold'] = False\n",
    "\n",
    "    if rebuild_tuple_flag or rebuild_gwb_flag:\n",
    "        combined_X = pd.concat([rec['X'] for rec in history_records], ignore_index=True)\n",
    "        combined_y = np.concatenate([rec['y'] for rec in history_records])\n",
    "        combined_buckets = assign_buckets(combined_X)\n",
    "        combined_gwb = None\n",
    "        if runtime_state.get('gwb_estimator') is not None:\n",
    "            if rebuild_gwb_flag:\n",
    "                runtime_state['gwb_estimator'].fit(\n",
    "                    combined_X.values,\n",
    "                    combined_y,\n",
    "                    categorical_values=combined_X[cat_cols] if cat_cols else None,\n",
    "                )\n",
    "                if '重建GWB索引' in actions_taken:\n",
    "                    idx = actions_taken.index('重建GWB索引')\n",
    "                    actions_taken[idx] = 'GWB索引已重建'\n",
    "                else:\n",
    "                    actions_taken.append('GWB索引已重建')\n",
    "            if runtime_state.get('use_gwb_weight', True):\n",
    "                combined_gwb = runtime_state['gwb_estimator'].predict_proba(\n",
    "                    combined_X.values,\n",
    "                    categorical_values=combined_X[cat_cols] if cat_cols else None,\n",
    "                )\n",
    "        if rebuild_tuple_flag:\n",
    "            new_refs = build_ref_tuples(\n",
    "                combined_X,\n",
    "                combined_y,\n",
    "                combined_buckets,\n",
    "                topk_per_class=cfg['REF_TUPLE'].get('topk_per_class', 256),\n",
    "                pos_quantile=cfg['REF_TUPLE'].get('pos_quantile', 0.7),\n",
    "                keep_history_ratio=runtime_state.get('keep_history_ratio', 0.3),\n",
    "                gwb_prob=combined_gwb,\n",
    "            )\n",
    "            runtime_state['ref_tuples'] = combine_history(\n",
    "                new_refs,\n",
    "                runtime_state.get('ref_tuples'),\n",
    "                runtime_state.get('keep_history_ratio', 0.3),\n",
    "            )\n",
    "            if '重建参考元组' in actions_taken:\n",
    "                idx = actions_taken.index('重建参考元组')\n",
    "                actions_taken[idx] = '参考元组已重建'\n",
    "            else:\n",
    "                actions_taken.append('参考元组已重建')\n",
    "        runtime_state['rebuild_ref_tuple'] = False\n",
    "        runtime_state['rebuild_gwb_index'] = False\n",
    "\n",
    "    similarity.configure({'sigma': runtime_state.get('sigma', 0.5)})\n",
    "\n",
    "    threshold_trace.append({\n",
    "        'window': int(month),\n",
    "        'alpha_star': float(alpha_star),\n",
    "        'beta_star': float(beta_star),\n",
    "        'alpha_smoothed': float(alpha_smooth),\n",
    "        'beta_smoothed': float(beta_smooth),\n",
    "        'objective_score': float(score_smoothed),\n",
    "        'bnd_ratio': bnd_ratio,\n",
    "        'pos_coverage': pos_cov,\n",
    "    })\n",
    "    window_metrics.append({\n",
    "        'window': int(month),\n",
    "        'psi': float(psi_val),\n",
    "        'tv': float(tv_val),\n",
    "        'posrate_gap': float(posrate_gap),\n",
    "        'posrate': posrate_curr,\n",
    "        'perf_drop': float(perf_drop),\n",
    "        'bnd_ratio': bnd_ratio,\n",
    "        'alpha': float(alpha_smooth),\n",
    "        'beta': float(beta_smooth),\n",
    "    })\n",
    "\n",
    "    print(\n",
    "        f\"【阈值选择】窗口={month} α*={alpha_star:.3f} β*={beta_star:.3f} 平滑后α={alpha_smooth:.3f} β={beta_smooth:.3f} \"\n",
    "        f\"BND占比={bnd_ratio:.3f} {metric_name}={score_smoothed:.4f}\"\n",
    "    )\n",
    "\n",
    "    if drift_level != 'NONE':\n",
    "        drift_events.append({\n",
    "            'window': int(month),\n",
    "            'level': drift_level,\n",
    "            'psi': float(psi_val),\n",
    "            'tv': float(tv_val),\n",
    "            'posrate_gap': float(posrate_gap),\n",
    "            'perf_drop': float(perf_drop),\n",
    "            'actions': '、'.join(actions_taken) if actions_taken else '无',\n",
    "        })\n",
    "        print(f\"【漂移】级别={drift_level}，动作={'、'.join(actions_taken) if actions_taken else '观察'}，窗口={month}\")\n",
    "        if drift_level == 'S3':\n",
    "            baseline_info['p_pos'] = P_block['p_pos']\n",
    "            baseline_info['posrate'] = posrate_curr\n",
    "            baseline_info['perf'] = score_smoothed\n",
    "\n",
    "    gwb_prob_block = None\n",
    "    if runtime_state.get('gwb_estimator') is not None and runtime_state.get('use_gwb_weight', True):\n",
    "        gwb_prob_block = runtime_state['gwb_estimator'].predict_proba(\n",
    "            X_block.values,\n",
    "            categorical_values=X_block[cat_cols] if cat_cols else None,\n",
    "        )\n",
    "    history_records.append({\n",
    "        'X': X_block.copy(),\n",
    "        'y': y_block.to_numpy(),\n",
    "        'buckets': buckets_block.copy(),\n",
    "        'gwb': gwb_prob_block,\n",
    "    })\n",
    "    if len(history_records) > max_history_windows:\n",
    "        history_records.pop(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_df = pd.DataFrame(threshold_trace)\n",
    "metrics_df = pd.DataFrame(window_metrics)\n",
    "drift_df = pd.DataFrame(drift_events)\n",
    "\n",
    "output_dir = Path(cfg['DATA']['data_dir']).resolve()\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "trace_path = output_dir / 'threshold_trace_v02.csv'\n",
    "metrics_path = output_dir / 'window_metrics.csv'\n",
    "drift_path = output_dir / 'drift_events.csv'\n",
    "\n",
    "trace_df.to_csv(trace_path, index=False)\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "drift_df.to_csv(drift_path, index=False)\n",
    "\n",
    "print(f\"【导出】阈值轨迹已保存到 {trace_path}\")\n",
    "print(f\"【导出】窗口指标已保存到 {metrics_path}\")\n",
    "print(f\"【导出】漂移事件已保存到 {drift_path}\")\n",
    "\n",
    "if not trace_df.empty:\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10, 12), sharex=True)\n",
    "    axes[0].plot(trace_df['window'], trace_df['alpha_smoothed'], label='α', marker='o')\n",
    "    axes[0].plot(trace_df['window'], trace_df['beta_smoothed'], label='β', marker='o')\n",
    "    axes[0].set_ylabel('阈值')\n",
    "    axes[0].set_title('α/β 轨迹（平滑后）')\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(metrics_df['window'], metrics_df['bnd_ratio'], color='tab:orange', marker='s')\n",
    "    axes[1].set_ylabel('BND占比')\n",
    "    axes[1].set_title('边界域占比')\n",
    "\n",
    "    axes[2].plot(metrics_df['window'], metrics_df['psi'], label='PSI', marker='^')\n",
    "    axes[2].plot(metrics_df['window'], metrics_df['tv'], label='TV', marker='v')\n",
    "    axes[2].set_ylabel('漂移指标')\n",
    "    axes[2].set_xlabel('窗口（月）')\n",
    "    axes[2].set_title('漂移指标轨迹')\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"【提示】流式窗口为空，未生成可视化图表。\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}