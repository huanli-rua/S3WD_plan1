{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4db9d230",
   "metadata": {},
   "source": [
    "# 00 · S3WD Baseline（中文）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "995b9506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【可视化字体】已设置为宋体优先（若系统缺失则自动回退）。\n",
      "【Python】 3.11.5\n",
      "【Pandas/Numpy】 2.0.3 1.26.4\n",
      "【模块路径】\n",
      "  zh_utils   -> e:\\yan\\组\\三支决策\\机器学习\\C三支决策与不平衡数据集分类\\S3WD实验\\s3wdlib\\zh_utils.py\n",
      "  data_io    -> e:\\yan\\组\\三支决策\\机器学习\\C三支决策与不平衡数据集分类\\S3WD实验\\s3wdlib\\data_io.py\n",
      "  features   -> e:\\yan\\组\\三支决策\\机器学习\\C三支决策与不平衡数据集分类\\S3WD实验\\s3wdlib\\features.py\n",
      "  kwb        -> e:\\yan\\组\\三支决策\\机器学习\\C三支决策与不平衡数据集分类\\S3WD实验\\s3wdlib\\kwb.py\n",
      "  objective  -> e:\\yan\\组\\三支决策\\机器学习\\C三支决策与不平衡数据集分类\\S3WD实验\\s3wdlib\\objective.py\n",
      "  trainer    -> e:\\yan\\组\\三支决策\\机器学习\\C三支决策与不平衡数据集分类\\S3WD实验\\s3wdlib\\trainer.py\n",
      "【函数签名】load_table_auto: (path: 'str', label_col: 'Optional[str | int]' = None, positive_label=1, continuous_label: 'Optional[str]' = None, threshold: 'Optional[float]' = None, threshold_op: 'str' = '>=') -> 'Tuple[pd.DataFrame, pd.Series]'\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# 0. 环境初始化与模块导入（中文）\n",
    "# ================================\n",
    "import os, sys, platform, importlib, inspect, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (f1_score, balanced_accuracy_score, precision_score, recall_score,\n",
    "                             matthews_corrcoef, cohen_kappa_score, roc_auc_score)\n",
    "# 1) 确保项目根目录在 sys.path[0]（notebooks/ 的上一级）\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# 2) 导入自研库（s3wdlib）各模块，并强制重载确保拿到最新版实现\n",
    "import s3wdlib.zh_utils as zh_utils\n",
    "import s3wdlib.data_io as data_io\n",
    "import s3wdlib.features as features\n",
    "import s3wdlib.kwb as kwb\n",
    "import s3wdlib.objective as objective\n",
    "import s3wdlib.trainer as trainer\n",
    "\n",
    "importlib.reload(zh_utils)\n",
    "importlib.reload(data_io)\n",
    "importlib.reload(features)\n",
    "importlib.reload(kwb)\n",
    "importlib.reload(objective)\n",
    "importlib.reload(trainer)\n",
    "\n",
    "# 3) 把常用符号直接引入（可读性更好）\n",
    "from s3wdlib.zh_utils import set_chinese_font, fix_minus\n",
    "from s3wdlib.data_io import load_table_auto, minmax_scale_fit_transform\n",
    "from s3wdlib.features import rank_features_mi, make_levels\n",
    "from s3wdlib.kwb import KWBProbEstimator\n",
    "from s3wdlib.objective import S3WDParams\n",
    "from s3wdlib.trainer import PSOParams, pso_learn_thresholds\n",
    "from s3wdlib.config_loader import load_yaml_cfg, extract_vars, show_cfg\n",
    "\n",
    "# 4) 可视化中文设置（宋体优先，负号正常）\n",
    "set_chinese_font(); fix_minus()\n",
    "print(\"【可视化字体】已设置为宋体优先（若系统缺失则自动回退）。\")\n",
    "\n",
    "# 5) 版本与路径自检（定位是否导入了正确文件）\n",
    "print(\"【Python】\", platform.python_version())\n",
    "print(\"【Pandas/Numpy】\", pd.__version__, np.__version__)\n",
    "print(\"【模块路径】\")\n",
    "print(\"  zh_utils   ->\", zh_utils.__file__)\n",
    "print(\"  data_io    ->\", data_io.__file__)\n",
    "print(\"  features   ->\", features.__file__)\n",
    "print(\"  kwb        ->\", kwb.__file__)\n",
    "print(\"  objective  ->\", objective.__file__)\n",
    "print(\"  trainer    ->\", trainer.__file__)\n",
    "print(\"【函数签名】load_table_auto:\", inspect.signature(data_io.load_table_auto))\n",
    "\n",
    "# 6) 随机种子（方便复现；如需完全一致可统一设置）\n",
    "np.random.seed(42)\n",
    "\n",
    "# 7) 警告精简（可选）\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3cdf5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【配置快照】\n",
      "- DATA: {'data_dir': '../data', 'data_file': 'airlines_train_regression_1000000.arff', 'continuous_label': 'DepDelay', 'threshold': 15, 'threshold_op': '>', 'label_col': None, 'positive_label': None, 'test_size': 0.3, 'val_size': 0.3, 'random_state': 42}\n",
      "- LEVEL: {'level_pcts': [0.6, 0.8, 1.0], 'ranker': 'mi'}\n",
      "- KWB: {'k': 6, 'metric': 'euclidean', 'eps': 1e-06}\n",
      "- S3WD: {'c1': 0.37, 'c2': 0.63, 'xi_min': 0.1, 'theta_pos': 0.9, 'theta_neg': 0.1, 'sigma': 3.0, 'regret_mode': 'utility', 'penalty_large': 1000000.0, 'gamma_last': True, 'gap': 0.02}\n",
      "- PSO: {'particles': 20, 'iters': 20, 'w_max': 0.9, 'w_min': 0.4, 'c1': 2.8, 'c2': 1.3, 'seed': 42}\n",
      "【参数就绪（来自 YAML）】 {'DATA_PATH': '../data\\\\airlines_train_regression_1000000.arff', 'label': 'DepDelay>15', 'splits': 'test=0.3, val=0.3, seed=42', 'kwb.k': 6, 'pso': {'particles': 20, 'iters': 20}}\n"
     ]
    }
   ],
   "source": [
    "# === 配置接入（YAML → dataclass → 变量字典）===\n",
    "# wine\n",
    "# CFG = load_yaml_cfg(\"../configs/s3wd_wine.yaml\")  # ← 如换配置文件，只改这里\n",
    "# Heart\n",
    "# CFG = load_yaml_cfg(\"../configs/s3wd_heart.yaml\")\n",
    "# Credit\n",
    "# CFG = load_yaml_cfg(\"../configs/s3wd_credit.yaml\")\n",
    "#airline\n",
    "CFG = load_yaml_cfg(\"../configs/s3wd_airline.yaml\")\n",
    "\n",
    "\n",
    "V   = extract_vars(CFG)\n",
    "show_cfg(CFG)\n",
    "\n",
    "# ✅ 兼容“连续列二值化”和“已有二值标签”两种配置\n",
    "if \"CONT_LABEL\" in V:\n",
    "    label_desc = f\"{V['CONT_LABEL']}{V['CONT_OP']}{V['CONT_THRESH']}\"\n",
    "elif \"LABEL_COL\" in V:\n",
    "    label_desc = f\"{V['LABEL_COL']}=={V.get('POSITIVE_LABEL', 1)}\"\n",
    "else:\n",
    "    label_desc = \"(未检测到标签配置)\"\n",
    "\n",
    "print(\"【参数就绪（来自 YAML）】\", {\n",
    "    \"DATA_PATH\": V[\"DATA_PATH\"],\n",
    "    \"label\": label_desc,\n",
    "    \"splits\": f\"test={V['TEST_SIZE']}, val={V['VAL_SIZE']}, seed={V['SEED']}\",\n",
    "    \"kwb.k\": V[\"KWB_K\"],\n",
    "    \"pso\": {\"particles\": V[\"PSO_particles\"], \"iters\": V[\"PSO_iters\"]}\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41be3dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【标签策略】连续列二值化：DepDelay > 15\n",
      "【数据加载完毕】样本数=1000000，特征数=9，正类比例=0.1559\n",
      "【数据加载】X_all, y_all = (1000000, 9) (1000000,)\n",
      "【数据切分】Xtr/Xte = (700000, 9) (300000, 9)\n"
     ]
    }
   ],
   "source": [
    "# === 读取数据（兼容“连续列二值化 / 已有二值标签”两种配置）+ 切分 ===\n",
    "from s3wdlib.data_io import load_table_auto\n",
    "\n",
    "# 统一参数打包（不存在的键用 .get() 不报错）\n",
    "kw = dict(\n",
    "    path=V[\"DATA_PATH\"],\n",
    "    label_col=V.get(\"LABEL_COL\"),\n",
    "    positive_label=V.get(\"POSITIVE_LABEL\"),\n",
    "    continuous_label=V.get(\"CONT_LABEL\"),\n",
    "    threshold=V.get(\"CONT_THRESH\"),\n",
    "    threshold_op=V.get(\"CONT_OP\"),\n",
    ")\n",
    "\n",
    "# 友好提示\n",
    "if V.get(\"CONT_LABEL\") is not None:\n",
    "    print(f\"【标签策略】连续列二值化：{V['CONT_LABEL']} {V['CONT_OP']} {V['CONT_THRESH']}\")\n",
    "elif V.get(\"LABEL_COL\") is not None:\n",
    "    print(f\"【标签策略】已有标签列：{V['LABEL_COL']} == {V.get('POSITIVE_LABEL', 1)} 视为正类\")\n",
    "else:\n",
    "    raise RuntimeError(\"未检测到标签配置（既无 CONT_* 也无 LABEL_COL）。请检查 YAML。\")\n",
    "\n",
    "# 读取数据\n",
    "X_all, y_all = load_table_auto(**kw)\n",
    "print(\"【数据加载】X_all, y_all =\", X_all.shape, y_all.shape)\n",
    "\n",
    "# 切分（与 YAML 一致）\n",
    "Xtr, Xte, ytr, yte = train_test_split(\n",
    "    X_all, y_all,\n",
    "    test_size=V[\"TEST_SIZE\"],\n",
    "    random_state=V[\"SEED\"],\n",
    "    stratify=y_all\n",
    ")\n",
    "print(\"【数据切分】Xtr/Xte =\", Xtr.shape, Xte.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9229e4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【分层复核】总特征=9 | L1=5 L2=7 L3=9\n"
     ]
    }
   ],
   "source": [
    "# 1) 训练集内切出验证集（仅用于阈值寻优）\n",
    "Xtr_sub, Xva, ytr_sub, yva = train_test_split(\n",
    "    Xtr, ytr, test_size=V[\"VAL_SIZE\"], stratify=ytr, random_state=V[\"SEED\"]\n",
    ")\n",
    "\n",
    "# 2) 归一化（仅在训练子集拟合）\n",
    "Xtr2, Xva2, scaler = minmax_scale_fit_transform(Xtr_sub, Xva)\n",
    "Xte2 = pd.DataFrame(scaler.transform(Xte), columns=Xte.columns)\n",
    "\n",
    "# 3) 分层（训练子集上）\n",
    "feat_rank, mi_vals = rank_features_mi(Xtr2, ytr_sub)\n",
    "L1, L2, L3 = make_levels(feat_rank)\n",
    "print(f\"【分层复核】总特征={len(feat_rank)} | L1={len(L1)} L2={len(L2)} L3={len(L3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22732d74",
   "metadata": {},
   "source": [
    "## KWB（Algorithm 1）训练与三层概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "878a9ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【KWB 完成】验证/测试三层概率就绪。\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4) KWB（训练子集拟合；验证/测试上出概率）\n",
    "kwb1 = KWBProbEstimator(\n",
    "        k=V[\"KWB_K\"],\n",
    "        metric=V[\"KWB_metric\"],\n",
    "        eps=V[\"KWB_eps\"],\n",
    "        use_faiss=V[\"KWB_use_faiss\"],\n",
    "        faiss_gpu=V[\"KWB_faiss_gpu\"],\n",
    "    ).fit(Xtr2[L1], ytr_sub)\n",
    "kwb2 = KWBProbEstimator(\n",
    "        k=V[\"KWB_K\"],\n",
    "        metric=V[\"KWB_metric\"],\n",
    "        eps=V[\"KWB_eps\"],\n",
    "        use_faiss=V[\"KWB_use_faiss\"],\n",
    "        faiss_gpu=V[\"KWB_faiss_gpu\"],\n",
    "    ).fit(Xtr2[L2], ytr_sub)\n",
    "kwb3 = KWBProbEstimator(\n",
    "        k=V[\"KWB_K\"],\n",
    "        metric=V[\"KWB_metric\"],\n",
    "        eps=V[\"KWB_eps\"],\n",
    "        use_faiss=V[\"KWB_use_faiss\"],\n",
    "        faiss_gpu=V[\"KWB_faiss_gpu\"],\n",
    "    ).fit(Xtr2[L3], ytr_sub)\n",
    "p1_va = kwb1.predict_proba(Xva2[L1]); p2_va = kwb2.predict_proba(Xva2[L2]); p3_va = kwb3.predict_proba(Xva2[L3])\n",
    "p1_te = kwb1.predict_proba(Xte2[L1]); p2_te = kwb2.predict_proba(Xte2[L2]); p3_te = kwb3.predict_proba(Xte2[L3])\n",
    "print(\"【KWB 完成】验证/测试三层概率就绪。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f8e4f8",
   "metadata": {},
   "source": [
    "## 验证集 PSO 学阈值（信息增益−后悔值 + 单调序 + ξ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59e73fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【PSO 学到阈值（验证集）】 ['α1=0.4608/β1=0.1935', 'α2=0.4608/β2=0.1935', 'α3=0.4608/β3=0.3395'] γ3=0.5000\n",
      "【适应度/约束】 {'fit': -0.009, 'pen_bnd': 0.0, 'pen_mono': 0.0}\n"
     ]
    }
   ],
   "source": [
    "# 5) 验证集 PSO 学阈值（信息增益−后悔值 + 单调序 + ξ）\n",
    "s3 = S3WDParams(\n",
    "    c1=V[\"S3_c1\"], c2=V[\"S3_c2\"], xi_min=V[\"S3_xi_min\"],\n",
    "    theta_pos=V[\"S3_theta_pos\"], theta_neg=V[\"S3_theta_neg\"],\n",
    "    penalty_large=V[\"S3_penalty_large\"],\n",
    "    gamma_last=V.get(\"S3_gamma_last\"),   # ← 用 gamma_last（True 或 0.5）\n",
    "    gap=V.get(\"S3_gap\", 0.02)\n",
    ")\n",
    "pso = PSOParams(\n",
    "    particles=V[\"PSO_particles\"], iters=V[\"PSO_iters\"],\n",
    "    w_max=V[\"PSO_w_max\"], w_min=V[\"PSO_w_min\"],\n",
    "    c1=V[\"PSO_c1\"], c2=V[\"PSO_c2\"], seed=V[\"PSO_seed\"], use_gpu=V[\"PSO_use_gpu\"]\n",
    ")\n",
    "(best_th, best_fit, detail) = pso_learn_thresholds([p1_va, p2_va, p3_va], yva.values, s3, pso)\n",
    "\n",
    "alphas, betas, gamma3 = best_th\n",
    "print(\"【PSO 学到阈值（验证集）】\", [f\"α{i+1}={a:.4f}/β{i+1}={b:.4f}\" for i,(a,b) in enumerate(zip(alphas,betas))], f\"γ3={gamma3:.4f}\")\n",
    "print(\"【适应度/约束】\", {\"fit\":round(best_fit,4), \"pen_bnd\":detail.get(\"pen_bnd\",None), \"pen_mono\":detail.get(\"pen_mono\",None)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769b5df6",
   "metadata": {},
   "source": [
    "## 测试集序贯三支决策 + 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53342cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【样本流转（学到的阈值）】L1 POS/BND/NEG = 22202 155788 122010  | L2 POS/BND/NEG = 13208 89126 53454  | L3 POS/NEG = 8144 80982\n",
      "【评估（测试集）】 {'F1': 0.2367, 'BAC': 0.5494, 'Prec': 0.2454, 'Rec': 0.2286, 'MCC': 0.1018, 'Kappa': 0.1017, 'AUC': 0.5494}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6) 测试集序贯三支决策 + 评估\n",
    "def _seq_predict_eval(p1, p2, p3, y_true, a1, b1, a2, b2, g3):\n",
    "    POS1 = (p1 >= a1); NEG1 = (p1 <= b1); BND1 = (~POS1) & (~NEG1)\n",
    "    p2s = p2[BND1]; POS2 = np.zeros_like(BND1, bool); NEG2 = np.zeros_like(BND1, bool)\n",
    "    POS2[BND1] = (p2s >= a2); NEG2[BND1] = (p2s <= b2)\n",
    "    BND2 = BND1 & (~POS2) & (~NEG2)\n",
    "    p3s = p3[BND2]; POS3 = np.zeros_like(BND2, bool); NEG3 = np.zeros_like(BND2, bool)\n",
    "    POS3[BND2] = (p3s >= g3); NEG3[BND2] = ~POS3[BND2]\n",
    "    y_hat = np.full_like(y_true, -1, int)\n",
    "    y_hat[POS1]=1; y_hat[NEG1]=0; y_hat[POS2]=1; y_hat[NEG2]=0; y_hat[POS3]=1; y_hat[NEG3]=0\n",
    "    flow = {\"L1\":(int(POS1.sum()), int(BND1.sum()), int(NEG1.sum())),\n",
    "            \"L2\":(int(POS2.sum()), int(BND2.sum()), int(NEG2.sum())),\n",
    "            \"L3\":(int(POS3.sum()), int(NEG3.sum()))}\n",
    "    return y_hat, flow\n",
    "\n",
    "a1,b1 = float(alphas[0]), float(betas[0])\n",
    "a2,b2 = float(alphas[1]), float(betas[1])\n",
    "g3    = float(gamma3)\n",
    "\n",
    "y_hat, flow = _seq_predict_eval(p1_te, p2_te, p3_te, yte.values,\n",
    "                           float(alphas[0]), float(betas[0]),\n",
    "                           float(alphas[1]), float(betas[1]),\n",
    "                           g3=0.5)\n",
    "print(\"【样本流转（学到的阈值）】L1 POS/BND/NEG =\", *flow[\"L1\"], \" | L2 POS/BND/NEG =\", *flow[\"L2\"], \" | L3 POS/NEG =\", *flow[\"L3\"])\n",
    "\n",
    "mask = (y_hat >= 0)\n",
    "metrics = {\n",
    "    'F1': round(f1_score(yte[mask], y_hat[mask]),4),\n",
    "    'BAC': round(balanced_accuracy_score(yte[mask], y_hat[mask]),4),\n",
    "    'Prec': round(precision_score(yte[mask], y_hat[mask]),4),\n",
    "    'Rec': round(recall_score(yte[mask], y_hat[mask]),4),\n",
    "    'MCC': round(matthews_corrcoef(yte[mask], y_hat[mask]),4),\n",
    "    'Kappa': round(cohen_kappa_score(yte[mask], y_hat[mask]),4),\n",
    "    'AUC': round(roc_auc_score(yte[mask], y_hat[mask]),4)\n",
    "}\n",
    "print(\"【评估（测试集）】\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad80fd",
   "metadata": {},
   "source": [
    "## 10 次独立 70/30 划分（Train→Val(寻优)→Test(评估)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d3fafc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【数据加载完毕】样本数=1000000，特征数=9，正类比例=0.1559\n",
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【数据加载完毕】样本数=1000000，特征数=9，正类比例=0.1559\n",
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【数据加载完毕】样本数=1000000，特征数=9，正类比例=0.1559\n",
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【数据加载完毕】样本数=1000000，特征数=9，正类比例=0.1559\n",
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【数据加载完毕】样本数=1000000，特征数=9，正类比例=0.1559\n",
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【数据加载完毕】样本数=1000000，特征数=9，正类比例=0.1559\n",
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【数据加载完毕】样本数=1000000，特征数=9，正类比例=0.1559\n",
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【数据加载完毕】样本数=1000000，特征数=9，正类比例=0.1559\n",
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【数据加载完毕】样本数=1000000，特征数=9，正类比例=0.1559\n",
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【数据加载完毕】样本数=1000000，特征数=9，正类比例=0.1559\n",
      "【归一化】已对训练/测试集进行 MinMax 缩放到 [0,1]。\n",
      "【10 次独立划分结果（前 5 行）】\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>BAC</th>\n",
       "      <th>Prec</th>\n",
       "      <th>Rec</th>\n",
       "      <th>MCC</th>\n",
       "      <th>Kappa</th>\n",
       "      <th>AUC</th>\n",
       "      <th>L1_POS</th>\n",
       "      <th>L1_BND</th>\n",
       "      <th>L1_NEG</th>\n",
       "      <th>...</th>\n",
       "      <th>L3_POS</th>\n",
       "      <th>L3_NEG</th>\n",
       "      <th>alpha1</th>\n",
       "      <th>beta1</th>\n",
       "      <th>alpha2</th>\n",
       "      <th>beta2</th>\n",
       "      <th>gamma3</th>\n",
       "      <th>fit</th>\n",
       "      <th>pen_bnd</th>\n",
       "      <th>pen_mono</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.2367</td>\n",
       "      <td>0.5494</td>\n",
       "      <td>0.2454</td>\n",
       "      <td>0.2286</td>\n",
       "      <td>0.1018</td>\n",
       "      <td>0.1017</td>\n",
       "      <td>0.5494</td>\n",
       "      <td>22202</td>\n",
       "      <td>155788</td>\n",
       "      <td>122010</td>\n",
       "      <td>...</td>\n",
       "      <td>8144</td>\n",
       "      <td>80982</td>\n",
       "      <td>0.4608</td>\n",
       "      <td>0.1935</td>\n",
       "      <td>0.4608</td>\n",
       "      <td>0.1935</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.0090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2352</td>\n",
       "      <td>0.5483</td>\n",
       "      <td>0.2423</td>\n",
       "      <td>0.2285</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>0.5483</td>\n",
       "      <td>22369</td>\n",
       "      <td>155536</td>\n",
       "      <td>122095</td>\n",
       "      <td>...</td>\n",
       "      <td>8290</td>\n",
       "      <td>82904</td>\n",
       "      <td>0.4603</td>\n",
       "      <td>0.1380</td>\n",
       "      <td>0.4603</td>\n",
       "      <td>0.1380</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.0087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2329</td>\n",
       "      <td>0.5467</td>\n",
       "      <td>0.2385</td>\n",
       "      <td>0.2276</td>\n",
       "      <td>0.0952</td>\n",
       "      <td>0.0952</td>\n",
       "      <td>0.5467</td>\n",
       "      <td>22325</td>\n",
       "      <td>158005</td>\n",
       "      <td>119670</td>\n",
       "      <td>...</td>\n",
       "      <td>8435</td>\n",
       "      <td>84056</td>\n",
       "      <td>0.4037</td>\n",
       "      <td>0.1902</td>\n",
       "      <td>0.4037</td>\n",
       "      <td>0.1902</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.0085</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2349</td>\n",
       "      <td>0.5482</td>\n",
       "      <td>0.2424</td>\n",
       "      <td>0.2278</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>0.0988</td>\n",
       "      <td>0.5482</td>\n",
       "      <td>22510</td>\n",
       "      <td>155652</td>\n",
       "      <td>121838</td>\n",
       "      <td>...</td>\n",
       "      <td>8175</td>\n",
       "      <td>80097</td>\n",
       "      <td>0.4895</td>\n",
       "      <td>0.2209</td>\n",
       "      <td>0.4895</td>\n",
       "      <td>0.2209</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.0096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2355</td>\n",
       "      <td>0.5484</td>\n",
       "      <td>0.2422</td>\n",
       "      <td>0.2292</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.0989</td>\n",
       "      <td>0.5484</td>\n",
       "      <td>22529</td>\n",
       "      <td>155655</td>\n",
       "      <td>121816</td>\n",
       "      <td>...</td>\n",
       "      <td>8251</td>\n",
       "      <td>80645</td>\n",
       "      <td>0.4762</td>\n",
       "      <td>0.1841</td>\n",
       "      <td>0.4762</td>\n",
       "      <td>0.1841</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.0086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       F1     BAC    Prec     Rec     MCC   Kappa     AUC  L1_POS  L1_BND  \\\n",
       "0  0.2367  0.5494  0.2454  0.2286  0.1018  0.1017  0.5494   22202  155788   \n",
       "1  0.2352  0.5483  0.2423  0.2285  0.0989  0.0989  0.5483   22369  155536   \n",
       "2  0.2329  0.5467  0.2385  0.2276  0.0952  0.0952  0.5467   22325  158005   \n",
       "3  0.2349  0.5482  0.2424  0.2278  0.0989  0.0988  0.5482   22510  155652   \n",
       "4  0.2355  0.5484  0.2422  0.2292  0.0990  0.0989  0.5484   22529  155655   \n",
       "\n",
       "   L1_NEG  ...  L3_POS  L3_NEG  alpha1   beta1  alpha2   beta2  gamma3  \\\n",
       "0  122010  ...    8144   80982  0.4608  0.1935  0.4608  0.1935     0.5   \n",
       "1  122095  ...    8290   82904  0.4603  0.1380  0.4603  0.1380     0.5   \n",
       "2  119670  ...    8435   84056  0.4037  0.1902  0.4037  0.1902     0.5   \n",
       "3  121838  ...    8175   80097  0.4895  0.2209  0.4895  0.2209     0.5   \n",
       "4  121816  ...    8251   80645  0.4762  0.1841  0.4762  0.1841     0.5   \n",
       "\n",
       "      fit  pen_bnd  pen_mono  \n",
       "0 -0.0090      0.0       0.0  \n",
       "1 -0.0087      0.0       0.0  \n",
       "2 -0.0085      0.0       0.0  \n",
       "3 -0.0096      0.0       0.0  \n",
       "4 -0.0086      0.0       0.0  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【均值 ± 标准差】\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.2354</td>\n",
       "      <td>0.0023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAC</th>\n",
       "      <td>0.5483</td>\n",
       "      <td>0.0014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prec</th>\n",
       "      <td>0.2422</td>\n",
       "      <td>0.0028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rec</th>\n",
       "      <td>0.2289</td>\n",
       "      <td>0.0021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCC</th>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kappa</th>\n",
       "      <td>0.0989</td>\n",
       "      <td>0.0030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUC</th>\n",
       "      <td>0.5483</td>\n",
       "      <td>0.0014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean     std\n",
       "F1     0.2354  0.0023\n",
       "BAC    0.5483  0.0014\n",
       "Prec   0.2422  0.0028\n",
       "Rec    0.2289  0.0021\n",
       "MCC    0.0990  0.0030\n",
       "Kappa  0.0989  0.0030\n",
       "AUC    0.5483  0.0014"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === 外层评测：10 次独立 70/30 划分（Train→Val(寻优)→Test(评估)）===\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from sklearn.metrics import (f1_score, balanced_accuracy_score, precision_score, recall_score,\n",
    "                             matthews_corrcoef, cohen_kappa_score, roc_auc_score)\n",
    "import numpy as np, pandas as pd, importlib\n",
    "\n",
    "# 确保拿到最新版目标/训练器实现\n",
    "import s3wdlib.objective as objective, s3wdlib.trainer as trainer\n",
    "importlib.reload(objective); importlib.reload(trainer)\n",
    "from s3wdlib.objective import S3WDParams\n",
    "from s3wdlib.trainer import PSOParams, pso_learn_thresholds\n",
    "from s3wdlib.data_io import minmax_scale_fit_transform, load_table_auto\n",
    "from s3wdlib.features import rank_features_mi, make_levels\n",
    "from s3wdlib.kwb import KWBProbEstimator\n",
    "\n",
    "def _seq_predict(p1, p2, p3, y_true, a1, b1, a2, b2, g3):\n",
    "    POS1 = (p1 >= a1); NEG1 = (p1 <= b1); BND1 = (~POS1) & (~NEG1)\n",
    "    p2s = p2[BND1]; POS2 = np.zeros_like(BND1, bool); NEG2 = np.zeros_like(BND1, bool)\n",
    "    POS2[BND1] = (p2s >= a2); NEG2[BND1] = (p2s <= b2)\n",
    "    BND2 = BND1 & (~POS2) & (~NEG2)\n",
    "    p3s = p3[BND2]; POS3 = np.zeros_like(BND2, bool); NEG3 = np.zeros_like(BND2, bool)\n",
    "    POS3[BND2] = (p3s >= g3); NEG3[BND2] = ~POS3[BND2]\n",
    "    y_hat = np.full_like(y_true, -1, int)\n",
    "    y_hat[POS1]=1; y_hat[NEG1]=0; y_hat[POS2]=1; y_hat[NEG2]=0; y_hat[POS3]=1; y_hat[NEG3]=0\n",
    "    flow = {\"L1\":(int(POS1.sum()), int(BND1.sum()), int(NEG1.sum())),\n",
    "            \"L2\":(int(POS2.sum()), int(BND2.sum()), int(NEG2.sum())),\n",
    "            \"L3\":(int(POS3.sum()), int(NEG3.sum()))}\n",
    "    return y_hat, flow\n",
    "\n",
    "def _safe_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def run_one_split(seed: int):\n",
    "    # === 读取全量数据（兼容 “连续列二值化 / 已有标签列”）===\n",
    "    kw = dict(\n",
    "        path=V[\"DATA_PATH\"],\n",
    "        label_col=V.get(\"LABEL_COL\"),\n",
    "        positive_label=V.get(\"POSITIVE_LABEL\"),\n",
    "        continuous_label=V.get(\"CONT_LABEL\"),\n",
    "        threshold=V.get(\"CONT_THRESH\"),\n",
    "        threshold_op=V.get(\"CONT_OP\"),\n",
    "    )\n",
    "    X_all, y_all = load_table_auto(**kw)\n",
    "\n",
    "    # === 外层一次 70/30 划分 ===\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=V[\"TEST_SIZE\"], random_state=seed)\n",
    "    (tr_idx, te_idx), = sss.split(X_all, y_all)\n",
    "    Xtr_all, Xte = X_all.iloc[tr_idx], X_all.iloc[te_idx]\n",
    "    ytr_all, yte  = y_all.iloc[tr_idx], y_all.iloc[te_idx]\n",
    "\n",
    "    # === 训练集再切 val（仅用于阈值寻优）===\n",
    "    Xtr, Xva, ytr, yva = train_test_split(\n",
    "        Xtr_all, ytr_all, test_size=V[\"VAL_SIZE\"], stratify=ytr_all, random_state=seed\n",
    "    )\n",
    "\n",
    "    # === 归一化仅在训练子集拟合 ===\n",
    "    Xtr2, Xva2, scaler = minmax_scale_fit_transform(Xtr, Xva)\n",
    "    Xte2 = pd.DataFrame(scaler.transform(Xte), columns=Xte.columns)\n",
    "\n",
    "    # === 分层在训练子集上确定（互信息）===\n",
    "    feat_rank, mi_vals = rank_features_mi(Xtr2, ytr)   # ← 修正 ytr_sub 未定义\n",
    "    L1, L2, L3 = make_levels(feat_rank, V.get(\"LEVEL_PCTS\", [0.6,0.8,1.0]))\n",
    "\n",
    "    # === KWB 训练（训练子集）与概率（val/test）===\n",
    "    k = int(V[\"KWB_K\"])\n",
    "    kwb1 = KWBProbEstimator(\n",
    "        k=k,\n",
    "        metric=V[\"KWB_metric\"],\n",
    "        eps=V[\"KWB_eps\"],\n",
    "        use_faiss=V[\"KWB_use_faiss\"],\n",
    "        faiss_gpu=V[\"KWB_faiss_gpu\"],\n",
    "    ).fit(Xtr2[L1], ytr)\n",
    "    kwb2 = KWBProbEstimator(\n",
    "        k=k,\n",
    "        metric=V[\"KWB_metric\"],\n",
    "        eps=V[\"KWB_eps\"],\n",
    "        use_faiss=V[\"KWB_use_faiss\"],\n",
    "        faiss_gpu=V[\"KWB_faiss_gpu\"],\n",
    "    ).fit(Xtr2[L2], ytr)\n",
    "    kwb3 = KWBProbEstimator(\n",
    "        k=k,\n",
    "        metric=V[\"KWB_metric\"],\n",
    "        eps=V[\"KWB_eps\"],\n",
    "        use_faiss=V[\"KWB_use_faiss\"],\n",
    "        faiss_gpu=V[\"KWB_faiss_gpu\"],\n",
    "    ).fit(Xtr2[L3], ytr)\n",
    "    p1_va = kwb1.predict_proba(Xva2[L1]); p2_va = kwb2.predict_proba(Xva2[L2]); p3_va = kwb3.predict_proba(Xva2[L3])\n",
    "    p1_te = kwb1.predict_proba(Xte2[L1]); p2_te = kwb2.predict_proba(Xte2[L2]); p3_te = kwb3.predict_proba(Xte2[L3])\n",
    "\n",
    "    # === 验证集上 PSO 学阈值 ===\n",
    "    s3 = S3WDParams(\n",
    "        c1=V[\"S3_c1\"], c2=V[\"S3_c2\"], xi_min=V[\"S3_xi_min\"],\n",
    "        theta_pos=V[\"S3_theta_pos\"], theta_neg=V[\"S3_theta_neg\"],\n",
    "        penalty_large=V[\"S3_penalty_large\"]  # 若你的 S3WDParams 支持 gamma_last/gap，这里再加\n",
    "    )\n",
    "    pso = PSOParams(\n",
    "        particles=V[\"PSO_particles\"], iters=V[\"PSO_iters\"],\n",
    "        w_max=V[\"PSO_w_max\"], w_min=V[\"PSO_w_min\"],\n",
    "        c1=V[\"PSO_c1\"], c2=V[\"PSO_c2\"], seed=seed, use_gpu=V[\"PSO_use_gpu\"]\n",
    "    )\n",
    "    (alphas, betas, gamma3), fit, detail = pso_learn_thresholds([p1_va, p2_va, p3_va], yva.values, s3, pso)\n",
    "\n",
    "    # === 测试集序贯三支决策 + 指标（MCC 传 y_true,y_pred；AUC 兜底）===\n",
    "    y_hat, flow = _seq_predict(p1_te, p2_te, p3_te, yte.values,\n",
    "                               float(alphas[0]), float(betas[0]),\n",
    "                               float(alphas[1]), float(betas[1]),\n",
    "                               float(gamma3))\n",
    "    mask = (y_hat >= 0)\n",
    "    yt, yp = yte[mask], y_hat[mask]\n",
    "    metrics = {\n",
    "        'F1':   f1_score(yt, yp),\n",
    "        'BAC':  balanced_accuracy_score(yt, yp),\n",
    "        'Prec': precision_score(yt, yp),\n",
    "        'Rec':  recall_score(yt, yp),\n",
    "        'MCC':  matthews_corrcoef(yt, yp),\n",
    "        'Kappa':cohen_kappa_score(yt, yp),\n",
    "        'AUC':  _safe_auc(yt, yp),\n",
    "    }\n",
    "    th = {'alpha1': float(alphas[0]), 'beta1': float(betas[0]),\n",
    "          'alpha2': float(alphas[1]), 'beta2': float(betas[1]), 'gamma3': float(gamma3)}\n",
    "    return metrics, flow, th, {'fit':float(fit), 'pen_bnd':detail.get('pen_bnd',0.0), 'pen_mono':detail.get('pen_mono',0.0)}\n",
    "\n",
    "# —— 跑 10 次 —— #\n",
    "rows = []\n",
    "base_seed = V.get(\"PSO_seed\", 42)\n",
    "for i in range(10):\n",
    "    m, flow, th, det = run_one_split(seed=base_seed + i)\n",
    "    rows.append({\n",
    "        **{k: (round(v,4) if isinstance(v, (int,float)) else v) for k,v in m.items()},\n",
    "        'L1_POS': flow['L1'][0], 'L1_BND': flow['L1'][1], 'L1_NEG': flow['L1'][2],\n",
    "        'L2_POS': flow['L2'][0], 'L2_BND': flow['L2'][1], 'L2_NEG': flow['L2'][2],\n",
    "        'L3_POS': flow['L3'][0], 'L3_NEG': flow['L3'][1],\n",
    "        **{k: round(v,4) for k,v in th.items()},\n",
    "        **{k: round(v,4) for k,v in det.items()}\n",
    "    })\n",
    "\n",
    "df_res = pd.DataFrame(rows)\n",
    "print(\"【10 次独立划分结果（前 5 行）】\")\n",
    "display(df_res.head())\n",
    "\n",
    "summary = pd.DataFrame({\n",
    "    'mean': df_res.mean(numeric_only=True).round(4),\n",
    "    'std':  df_res.std(numeric_only=True).round(4)\n",
    "})\n",
    "print(\"【均值 ± 标准差】\")\n",
    "display(summary.loc[['F1','BAC','Prec','Rec','MCC','Kappa','AUC']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "719df993",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary.to_excel('../targets/airline.xlsx')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}