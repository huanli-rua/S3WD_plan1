{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4db9d230",
   "metadata": {},
   "source": [
    "# 00 · S3WD Baseline（中文）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b9506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 0. 环境初始化与模块导入（中文）\n",
    "# ================================\n",
    "import os, sys, platform, importlib, inspect, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (f1_score, balanced_accuracy_score, precision_score, recall_score,\n",
    "                             matthews_corrcoef, cohen_kappa_score, roc_auc_score)\n",
    "# 1) 确保项目根目录在 sys.path[0]（notebooks/ 的上一级）\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# 2) 导入自研库（s3wdlib）各模块，并强制重载确保拿到最新版实现\n",
    "import s3wdlib.zh_utils as zh_utils\n",
    "import s3wdlib.data_io as data_io\n",
    "import s3wdlib.features as features\n",
    "import s3wdlib.gwb as gwb\n",
    "import s3wdlib.objective as objective\n",
    "import s3wdlib.trainer as trainer\n",
    "\n",
    "importlib.reload(zh_utils)\n",
    "importlib.reload(data_io)\n",
    "importlib.reload(features)\n",
    "importlib.reload(gwb)\n",
    "importlib.reload(objective)\n",
    "importlib.reload(trainer)\n",
    "\n",
    "# 3) 把常用符号直接引入（可读性更好）\n",
    "from s3wdlib.zh_utils import set_chinese_font, fix_minus\n",
    "from s3wdlib.data_io import load_table_auto, minmax_scale_fit_transform\n",
    "from s3wdlib.features import rank_features_mi, make_levels\n",
    "from s3wdlib.gwb import GWBProbEstimator\n",
    "from s3wdlib.objective import S3WDParams\n",
    "from s3wdlib.trainer import PSOParams, pso_learn_thresholds\n",
    "from s3wdlib.config_loader import load_yaml_cfg, extract_vars, show_cfg\n",
    "\n",
    "# 4) 可视化中文设置（宋体优先，负号正常）\n",
    "set_chinese_font(); fix_minus()\n",
    "print(\"【可视化字体】已设置为宋体优先（若系统缺失则自动回退）。\")\n",
    "\n",
    "# 5) 版本与路径自检（定位是否导入了正确文件）\n",
    "print(\"【Python】\", platform.python_version())\n",
    "print(\"【Pandas/Numpy】\", pd.__version__, np.__version__)\n",
    "print(\"【模块路径】\")\n",
    "print(\"  zh_utils   ->\", zh_utils.__file__)\n",
    "print(\"  data_io    ->\", data_io.__file__)\n",
    "print(\"  features   ->\", features.__file__)\n",
    "print(\"  gwb        ->\", gwb.__file__)\n",
    "print(\"  objective  ->\", objective.__file__)\n",
    "print(\"  trainer    ->\", trainer.__file__)\n",
    "print(\"【函数签名】load_table_auto:\", inspect.signature(data_io.load_table_auto))\n",
    "\n",
    "# 6) 随机种子（方便复现；如需完全一致可统一设置）\n",
    "np.random.seed(42)\n",
    "\n",
    "# 7) 警告精简（可选）\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cdf5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 配置接入（YAML → dataclass → 变量字典）===\n",
    "# wine\n",
    "# CFG = load_yaml_cfg(\"../configs/s3wd_wine.yaml\")  # ← 如换配置文件，只改这里\n",
    "# Heart\n",
    "CFG = load_yaml_cfg(\"../configs/s3wd_heart.yaml\")\n",
    "# Credit\n",
    "# CFG = load_yaml_cfg(\"../configs/s3wd_credit.yaml\")\n",
    "# airline\n",
    "# CFG = load_yaml_cfg(\"../configs/s3wd_airline.yaml\")\n",
    "\n\n",
    "V   = extract_vars(CFG)\n",
    "show_cfg(CFG)\n",
    "\n",
    "# ✅ 兼容“连续列二值化”和“已有二值标签”两种配置\n",
    "if \"CONT_LABEL\" in V:\n",
    "    label_desc = f\"{V['CONT_LABEL']}{V['CONT_OP']}{V['CONT_THRESH']}\"\n",
    "elif \"LABEL_COL\" in V:\n",
    "    label_desc = f\"{V['LABEL_COL']}=={V.get('POSITIVE_LABEL', 1)}\"\n",
    "else:\n",
    "    label_desc = \"(未检测到标签配置)\"\n",
    "\n",
    "gwb_desc = {\n",
    "    \"k\": V[\"GWB_K\"],\n",
    "    \"mode\": V.get(\"GWB_mode\"),\n",
    "    \"bandwidth\": V.get(\"GWB_bandwidth\"),\n",
    "    \"use_faiss\": bool(V.get(\"GWB_use_faiss\", False)),\n",
    "    \"faiss_gpu\": bool(V.get(\"GWB_faiss_gpu\", False)),\n",
    "}\n",
    "print(\"【参数就绪（来自 YAML）】\", {\n",
    "    \"DATA_PATH\": V[\"DATA_PATH\"],\n",
    "    \"label\": label_desc,\n",
    "    \"splits\": f\"test={V['TEST_SIZE']}, val={V['VAL_SIZE']}, seed={V['RANDOM_STATE']}\",\n",
    "    \"gwb\": gwb_desc,\n",
    "    \"pso\": {\"particles\": V[\"PSO_particles\"], \"iters\": V[\"PSO_iters\"]}\n",
    "})\n",
    "\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41be3dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【标签策略】连续列二值化：DepDelay > 15\n",
      "【数据加载完毕】样本数=1000000，特征数=9，正类比例=0.1559\n",
      "【数据加载】X_all, y_all = (1000000, 9) (1000000,)\n",
      "【数据切分】Xtr/Xte = (700000, 9) (300000, 9)\n"
     ]
    }
   ],
   "source": [
    "# === 读取数据（兼容“连续列二值化 / 已有二值标签”两种配置）+ 切分 ===\n",
    "from s3wdlib.data_io import load_table_auto\n",
    "\n",
    "# 统一参数打包（不存在的键用 .get() 不报错）\n",
    "kw = dict(\n",
    "    path=V[\"DATA_PATH\"],\n",
    "    label_col=V.get(\"LABEL_COL\"),\n",
    "    positive_label=V.get(\"POSITIVE_LABEL\"),\n",
    "    continuous_label=V.get(\"CONT_LABEL\"),\n",
    "    threshold=V.get(\"CONT_THRESH\"),\n",
    "    threshold_op=V.get(\"CONT_OP\"),\n",
    ")\n",
    "\n",
    "# 友好提示\n",
    "if V.get(\"CONT_LABEL\") is not None:\n",
    "    print(f\"【标签策略】连续列二值化：{V['CONT_LABEL']} {V['CONT_OP']} {V['CONT_THRESH']}\")\n",
    "elif V.get(\"LABEL_COL\") is not None:\n",
    "    print(f\"【标签策略】已有标签列：{V['LABEL_COL']} == {V.get('POSITIVE_LABEL', 1)} 视为正类\")\n",
    "else:\n",
    "    raise RuntimeError(\"未检测到标签配置（既无 CONT_* 也无 LABEL_COL）。请检查 YAML。\")\n",
    "\n",
    "# 读取数据（保持原始顺序）\n",
    "X_all, y_all = load_table_auto(**kw)\n",
    "print(\"【数据加载】X_all, y_all =\", X_all.shape, y_all.shape)\n",
    "\n",
    "# 按时间顺序划分 70/30（不打乱）\n",
    "test_size = float(V[\"TEST_SIZE\"])\n",
    "cut = int((1.0 - test_size) * len(X_all))\n",
    "cut = max(1, min(len(X_all) - 1, cut))\n",
    "Xtr, Xte = X_all.iloc[:cut], X_all.iloc[cut:]\n",
    "ytr, yte = y_all.iloc[:cut], y_all.iloc[cut:]\n",
    "print(\"【划分方式】按时间顺序：train=%d, test=%d\" % (len(Xtr), len(Xte)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9229e4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 训练集内切出验证集（按时间顺序，仅用于阈值寻优）\n",
    "val_size = float(V[\"VAL_SIZE\"])\n",
    "if 0 < val_size < 1:\n",
    "    vcut = int((1.0 - val_size) * len(Xtr))\n",
    "    vcut = max(1, min(len(Xtr) - 1, vcut))\n",
    "    Xtr_sub, Xva = Xtr.iloc[:vcut], Xtr.iloc[vcut:]\n",
    "    ytr_sub, yva = ytr.iloc[:vcut], ytr.iloc[vcut:]\n",
    "else:\n",
    "    raise ValueError(\"VAL_SIZE 需在 (0,1) 内以进行验证划分\")\n",
    "\n",
    "# 2) 归一化（仅在训练子集拟合）\n",
    "Xtr2, Xva2, scaler = minmax_scale_fit_transform(Xtr_sub, Xva)\n",
    "Xte2 = pd.DataFrame(scaler.transform(Xte), columns=Xte.columns)\n",
    "\n",
    "# 3) 分层（训练子集上）\n",
    "feat_rank, mi_vals = rank_features_mi(Xtr2, ytr_sub)\n",
    "L1, L2, L3 = make_levels(feat_rank)\n",
    "print(f\"【分层复核】总特征={len(feat_rank)} | L1={len(L1)} L2={len(L2)} L3={len(L3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22732d74",
   "metadata": {},
   "source": [
    "## GWB（Algorithm 1）训练与三层概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878a9ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gwb_kwargs = {\n",
    "    \"k\": int(V[\"GWB_K\"]),\n",
    "    \"mode\": V.get(\"GWB_mode\", \"epanechnikov\"),\n",
    "    \"bandwidth\": V.get(\"GWB_bandwidth\"),\n",
    "    \"bandwidth_scale\": V.get(\"GWB_bandwidth_scale\", 1.0),\n",
    "    \"use_faiss\": bool(V.get(\"GWB_use_faiss\", False)),\n",
    "    \"faiss_gpu\": bool(V.get(\"GWB_faiss_gpu\", False)),\n",
    "}\n",
    "gwb_kwargs = {k: v for k, v in gwb_kwargs.items() if v is not None}\n",
    "gwb1 = GWBProbEstimator(**gwb_kwargs).fit(Xtr2[L1], ytr_sub)\n",
    "gwb2 = GWBProbEstimator(**gwb_kwargs).fit(Xtr2[L2], ytr_sub)\n",
    "gwb3 = GWBProbEstimator(**gwb_kwargs).fit(Xtr2[L3], ytr_sub)\n",
    "p1_va = gwb1.predict_proba(Xva2[L1]); p2_va = gwb2.predict_proba(Xva2[L2]); p3_va = gwb3.predict_proba(Xva2[L3])\n",
    "p1_te = gwb1.predict_proba(Xte2[L1]); p2_te = gwb2.predict_proba(Xte2[L2]); p3_te = gwb3.predict_proba(Xte2[L3])\n",
    "print(\"【GWB 完成】验证/测试三层概率就绪。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f8e4f8",
   "metadata": {},
   "source": [
    "## 验证集 PSO 学阈值（信息增益−后悔值 + 单调序 + ξ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e73fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) 验证集 PSO 学阈值（信息增益−后悔值 + 单调序 + ξ）\n",
    "s3 = S3WDParams(\n",
    "    c1=V[\"S3_c1\"], c2=V[\"S3_c2\"], xi_min=V[\"S3_xi_min\"],\n",
    "    theta_pos=V[\"S3_theta_pos\"], theta_neg=V[\"S3_theta_neg\"],\n",
    "    penalty_large=V[\"S3_penalty_large\"],\n",
    "    gamma_last=V.get(\"S3_gamma_last\"),   # ← 用 gamma_last（True 或 0.5）\n",
    "    gap=V.get(\"S3_gap\", 0.02)\n",
    ")\n",
    "pso = PSOParams(\n",
    "    particles=V[\"PSO_particles\"], iters=V[\"PSO_iters\"],\n",
    "    w_max=V[\"PSO_w_max\"], w_min=V[\"PSO_w_min\"],\n",
    "    c1=V[\"PSO_c1\"], c2=V[\"PSO_c2\"], seed=V[\"PSO_seed\"], use_gpu=V[\"PSO_use_gpu\"]\n",
    ")\n",
    "(best_th, best_fit, detail) = pso_learn_thresholds([p1_va, p2_va, p3_va], yva.values, s3, pso)\n",
    "\n",
    "alphas, betas, gamma3 = best_th\n",
    "print(\"【PSO 学到阈值（验证集）】\", [f\"α{i+1}={a:.4f}/β{i+1}={b:.4f}\" for i,(a,b) in enumerate(zip(alphas,betas))], f\"γ3={gamma3:.4f}\")\n",
    "print(\"【适应度/约束】\", {\"fit\":round(best_fit,4), \"pen_bnd\":detail.get(\"pen_bnd\",None), \"pen_mono\":detail.get(\"pen_mono\",None)})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769b5df6",
   "metadata": {},
   "source": [
    "## 测试集序贯三支决策 + 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53342cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6) 测试集序贯三支决策 + 评估\n",
    "def _seq_predict_eval(p1, p2, p3, y_true, a1, b1, a2, b2, g3):\n",
    "    POS1 = (p1 >= a1); NEG1 = (p1 <= b1); BND1 = (~POS1) & (~NEG1)\n",
    "    p2s = p2[BND1]; POS2 = np.zeros_like(BND1, bool); NEG2 = np.zeros_like(BND1, bool)\n",
    "    POS2[BND1] = (p2s >= a2); NEG2[BND1] = (p2s <= b2)\n",
    "    BND2 = BND1 & (~POS2) & (~NEG2)\n",
    "    p3s = p3[BND2]; POS3 = np.zeros_like(BND2, bool); NEG3 = np.zeros_like(BND2, bool)\n",
    "    POS3[BND2] = (p3s >= g3); NEG3[BND2] = ~POS3[BND2]\n",
    "    y_hat = np.full_like(y_true, -1, int)\n",
    "    y_hat[POS1]=1; y_hat[NEG1]=0; y_hat[POS2]=1; y_hat[NEG2]=0; y_hat[POS3]=1; y_hat[NEG3]=0\n",
    "    flow = {\"L1\":(int(POS1.sum()), int(BND1.sum()), int(NEG1.sum())),\n",
    "            \"L2\":(int(POS2.sum()), int(BND2.sum()), int(NEG2.sum())),\n",
    "            \"L3\":(int(POS3.sum()), int(NEG3.sum()))}\n",
    "    return y_hat, flow\n",
    "\n",
    "a1,b1 = float(alphas[0]), float(betas[0])\n",
    "a2,b2 = float(alphas[1]), float(betas[1])\n",
    "g3    = float(gamma3)\n",
    "\n",
    "y_hat, flow = _seq_predict_eval(p1_te, p2_te, p3_te, yte.values,\n",
    "                           float(alphas[0]), float(betas[0]),\n",
    "                           float(alphas[1]), float(betas[1]),\n",
    "                           g3=0.5)\n",
    "print(\"【样本流转（学到的阈值）】L1 POS/BND/NEG =\", *flow[\"L1\"], \" | L2 POS/BND/NEG =\", *flow[\"L2\"], \" | L3 POS/NEG =\", *flow[\"L3\"])\n",
    "\n",
    "mask = (y_hat >= 0)\n",
    "metrics = {\n",
    "    'F1': round(f1_score(yte[mask], y_hat[mask]),4),\n",
    "    'BAC': round(balanced_accuracy_score(yte[mask], y_hat[mask]),4),\n",
    "    'Prec': round(precision_score(yte[mask], y_hat[mask]),4),\n",
    "    'Rec': round(recall_score(yte[mask], y_hat[mask]),4),\n",
    "    'MCC': round(matthews_corrcoef(yte[mask], y_hat[mask]),4),\n",
    "    'Kappa': round(cohen_kappa_score(yte[mask], y_hat[mask]),4),\n",
    "    'AUC': round(roc_auc_score(yte[mask], y_hat[mask]),4)\n",
    "}\n",
    "print(\"【评估（测试集）】\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad80fd",
   "metadata": {},
   "source": [
    "## 按时间顺序单次 70/30 划分（Train→Val→Test）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3fafc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 外层评测：按时间顺序单次 70/30 划分（Train→Val→Test）===\n",
    "from sklearn.metrics import (f1_score, balanced_accuracy_score, precision_score, recall_score,\n",
    "                             matthews_corrcoef, cohen_kappa_score, roc_auc_score)\n",
    "import numpy as np, pandas as pd, importlib\n",
    "\n",
    "# 确保拿到最新版目标/训练器实现\n",
    "import s3wdlib.objective as objective, s3wdlib.trainer as trainer\n",
    "importlib.reload(objective); importlib.reload(trainer)\n",
    "from s3wdlib.objective import S3WDParams\n",
    "from s3wdlib.trainer import PSOParams, pso_learn_thresholds\n",
    "from s3wdlib.data_io import minmax_scale_fit_transform, load_table_auto\n",
    "from s3wdlib.features import rank_features_mi, make_levels\n",
    "from s3wdlib.gwb import GWBProbEstimator\n",
    "\n",
    "\n",
    "def _seq_predict(p1, p2, p3, y_true, a1, b1, a2, b2, g3):\n",
    "    POS1 = (p1 >= a1); NEG1 = (p1 <= b1); BND1 = (~POS1) & (~NEG1)\n",
    "    p2s = p2[BND1]; POS2 = np.zeros_like(BND1, bool); NEG2 = np.zeros_like(BND1, bool)\n",
    "    POS2[BND1] = (p2s >= a2); NEG2[BND1] = (p2s <= b2)\n",
    "    BND2 = BND1 & (~POS2) & (~NEG2)\n",
    "    p3s = p3[BND2]; POS3 = np.zeros_like(BND2, bool); NEG3 = np.zeros_like(BND2, bool)\n",
    "    POS3[BND2] = (p3s >= g3); NEG3[BND2] = ~POS3[BND2]\n",
    "    y_hat = np.full_like(y_true, -1, int)\n",
    "    y_hat[POS1]=1; y_hat[NEG1]=0; y_hat[POS2]=1; y_hat[NEG2]=0; y_hat[POS3]=1; y_hat[NEG3]=0\n",
    "    flow = {\"L1\":(int(POS1.sum()), int(BND1.sum()), int(NEG1.sum())),\n",
    "            \"L2\":(int(POS2.sum()), int(BND2.sum()), int(NEG2.sum())),\n",
    "            \"L3\":(int(POS3.sum()), int(NEG3.sum()))}\n",
    "    return y_hat, flow\n",
    "\n",
    "\n",
    "def _safe_auc(y_true, y_pred):\n",
    "    try:\n",
    "        return roc_auc_score(y_true, y_pred)\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "\n",
    "def run_one_split(seed: int):\n",
    "    # === 读取全量数据（保持时间顺序）===\n",
    "    kw = dict(\n",
    "        path=V[\"DATA_PATH\"],\n",
    "        label_col=V.get(\"LABEL_COL\"),\n",
    "        positive_label=V.get(\"POSITIVE_LABEL\"),\n",
    "        continuous_label=V.get(\"CONT_LABEL\"),\n",
    "        threshold=V.get(\"CONT_THRESH\"),\n",
    "        threshold_op=V.get(\"CONT_OP\"),\n",
    "    )\n",
    "    X_all, y_all = load_table_auto(**kw)\n",
    "\n",
    "    # === 外层一次 70/30 按时间顺序划分 ===\n",
    "    test_size = float(V[\"TEST_SIZE\"])\n",
    "    cut = int((1.0 - test_size) * len(X_all))\n",
    "    cut = max(1, min(len(X_all) - 1, cut))\n",
    "    Xtr_all, Xte = X_all.iloc[:cut], X_all.iloc[cut:]\n",
    "    ytr_all, yte  = y_all.iloc[:cut], y_all.iloc[cut:]\n",
    "\n",
    "    # === 训练集再按时间顺序切验证集（仅用于阈值寻优）===\n",
    "    val_size = float(V[\"VAL_SIZE\"])\n",
    "    if 0 < val_size < 1:\n",
    "        vcut = int((1.0 - val_size) * len(Xtr_all))\n",
    "        vcut = max(1, min(len(Xtr_all) - 1, vcut))\n",
    "        Xtr, Xva = Xtr_all.iloc[:vcut], Xtr_all.iloc[vcut:]\n",
    "        ytr, yva = ytr_all.iloc[:vcut], ytr_all.iloc[vcut:]\n",
    "    else:\n",
    "        raise ValueError(\"VAL_SIZE 需在 (0,1) 内以进行验证划分\")\n",
    "\n",
    "    # === 归一化仅在训练子集拟合 ===\n",
    "    Xtr2, Xva2, scaler = minmax_scale_fit_transform(Xtr, Xva)\n",
    "    Xte2 = pd.DataFrame(scaler.transform(Xte), columns=Xte.columns)\n",
    "\n",
    "    # === 分层在训练子集上确定（互信息）===\n",
    "    feat_rank, mi_vals = rank_features_mi(Xtr2, ytr)\n",
    "    L1, L2, L3 = make_levels(feat_rank, V.get(\"LEVEL_PCTS\", [0.6,0.8,1.0]))\n",
    "\n",
    "    # === GWB 训练（训练子集）与概率（val/test）===\n",
    "    gwb_kwargs = {\n",
    "        \"k\": int(V[\"GWB_K\"]),\n",
    "        \"mode\": V.get(\"GWB_mode\", \"epanechnikov\"),\n",
    "        \"bandwidth\": V.get(\"GWB_bandwidth\"),\n",
    "        \"bandwidth_scale\": V.get(\"GWB_bandwidth_scale\", 1.0),\n",
    "        \"use_faiss\": bool(V.get(\"GWB_use_faiss\", False)),\n",
    "        \"faiss_gpu\": bool(V.get(\"GWB_faiss_gpu\", False)),\n",
    "    }\n",
    "    gwb_kwargs = {k: v for k, v in gwb_kwargs.items() if v is not None}\n",
    "    gwb1 = GWBProbEstimator(**gwb_kwargs).fit(Xtr2[L1], ytr)\n",
    "    gwb2 = GWBProbEstimator(**gwb_kwargs).fit(Xtr2[L2], ytr)\n",
    "    gwb3 = GWBProbEstimator(**gwb_kwargs).fit(Xtr2[L3], ytr)\n",
    "    p1_va = gwb1.predict_proba(Xva2[L1]); p2_va = gwb2.predict_proba(Xva2[L2]); p3_va = gwb3.predict_proba(Xva2[L3])\n",
    "    p1_te = gwb1.predict_proba(Xte2[L1]); p2_te = gwb2.predict_proba(Xte2[L2]); p3_te = gwb3.predict_proba(Xte2[L3])\n",
    "\n",
    "    # === 验证集上 PSO 学阈值 ===\n",
    "    s3 = S3WDParams(\n",
    "        c1=V[\"S3_c1\"], c2=V[\"S3_c2\"], xi_min=V[\"S3_xi_min\"],\n",
    "        theta_pos=V[\"S3_theta_pos\"], theta_neg=V[\"S3_theta_neg\"],\n",
    "        penalty_large=V[\"S3_penalty_large\"]\n",
    "    )\n",
    "    pso = PSOParams(\n",
    "        particles=V[\"PSO_particles\"], iters=V[\"PSO_iters\"],\n",
    "        w_max=V[\"PSO_w_max\"], w_min=V[\"PSO_w_min\"],\n",
    "        c1=V[\"PSO_c1\"], c2=V[\"PSO_c2\"], seed=seed, use_gpu=V[\"PSO_use_gpu\"]\n",
    "    )\n",
    "    (alphas, betas, gamma3), fit, detail = pso_learn_thresholds([p1_va, p2_va, p3_va], yva.values, s3, pso)\n",
    "\n",
    "    # === 测试集序贯三支决策 + 指标（MCC 传 y_true,y_pred；AUC 兜底）===\n",
    "    y_hat, flow = _seq_predict(p1_te, p2_te, p3_te, yte.values,\n",
    "                               float(alphas[0]), float(betas[0]),\n",
    "                               float(alphas[1]), float(betas[1]),\n",
    "                               float(gamma3))\n",
    "    mask = (y_hat >= 0)\n",
    "    yt, yp = yte[mask], y_hat[mask]\n",
    "    metrics = {\n",
    "        'F1':   f1_score(yt, yp),\n",
    "        'BAC':  balanced_accuracy_score(yt, yp),\n",
    "        'Prec': precision_score(yt, yp),\n",
    "        'Rec':  recall_score(yt, yp),\n",
    "        'MCC':  matthews_corrcoef(yt, yp),\n",
    "        'Kappa':cohen_kappa_score(yt, yp),\n",
    "        'AUC':  _safe_auc(yt, yp),\n",
    "    }\n",
    "    th = {'alpha1': float(alphas[0]), 'beta1': float(betas[0]),\n",
    "          'alpha2': float(alphas[1]), 'beta2': float(betas[1]), 'gamma3': float(gamma3)}\n",
    "    return metrics, flow, th, {'fit':float(fit), 'pen_bnd':detail.get('pen_bnd',0.0), 'pen_mono':detail.get('pen_mono',0.0)}\n",
    "\n",
    "\n",
    "base_seed = V.get(\"PSO_seed\", 42)\n",
    "metrics, flow, th, det = run_one_split(seed=base_seed)\n",
    "\n",
    "row = {\n",
    "    **{k: (round(v,4) if isinstance(v, (int,float)) else v) for k,v in metrics.items()},\n",
    "    'L1_POS': flow['L1'][0], 'L1_BND': flow['L1'][1], 'L1_NEG': flow['L1'][2],\n",
    "    'L2_POS': flow['L2'][0], 'L2_BND': flow['L2'][1], 'L2_NEG': flow['L2'][2],\n",
    "    'L3_POS': flow['L3'][0], 'L3_NEG': flow['L3'][1],\n",
    "    **{k: round(v,4) for k,v in th.items()},\n",
    "    **{k: round(v,4) for k,v in det.items()}\n",
    "}\n",
    "df_res = pd.DataFrame([row])\n",
    "print(\"【单次按时间顺序划分结果】\")\n",
    "display(df_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719df993",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary.to_excel('../targets/heart.xlsx')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
