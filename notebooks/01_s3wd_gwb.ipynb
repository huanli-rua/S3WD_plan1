{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 · S3WD-GWB 动态循环实验（Airline）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3221ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 · 环境初始化与依赖检查\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import platform\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "except Exception:\n",
    "    def display(obj):\n",
    "        print(obj)\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "RUNTIME_INFO = {\n",
    "    'python': sys.version.split()[0],\n",
    "    'platform': platform.platform(),\n",
    "    'project_root': str(PROJECT_ROOT),\n",
    "}\n",
    "print('【步骤0摘要】已初始化运行环境：', RUNTIME_INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943e4c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 · 载入配置并展开变量\n",
    "from s3wdlib.config_loader import load_yaml_cfg, extract_vars, show_cfg\n",
    "\n",
    "CONFIG_PATH = PROJECT_ROOT / 'configs' / 's3wd_airline_dynamic.yaml'\n",
    "CFG = load_yaml_cfg(str(CONFIG_PATH))\n",
    "V = extract_vars(CFG)\n",
    "show_cfg(CFG)\n",
    "print('【步骤1摘要】配置文件加载完成，关键键数=', len(V))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db89d567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 · 加载数据集并准备标签\n",
    "from s3wdlib.data_io import load_table_auto\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "raw_data_path = Path(V['DATA_PATH'])\n",
    "if not raw_data_path.is_absolute():\n",
    "    data_path = (CONFIG_PATH.parent / raw_data_path).resolve()\n",
    "else:\n",
    "    data_path = raw_data_path\n",
    "\n",
    "if data_path.exists():\n",
    "    X_df, y_sr = load_table_auto(\n",
    "        str(data_path),\n",
    "        label_col=V.get('LABEL_COL'),\n",
    "        positive_label=V.get('POSITIVE_LABEL', 1),\n",
    "        continuous_label=V.get('CONT_LABEL'),\n",
    "        threshold=V.get('CONT_THRESH'),\n",
    "        threshold_op=V.get('CONT_OP', '>='),\n",
    "    )\n",
    "    data_source = f'航空延误真实数据: {data_path.name}'\n",
    "else:\n",
    "    n_samples = 12000\n",
    "    n_features = 32\n",
    "    X_arr, y_arr = make_classification(\n",
    "        n_samples=n_samples,\n",
    "        n_features=n_features,\n",
    "        n_informative=18,\n",
    "        n_redundant=6,\n",
    "        n_repeated=0,\n",
    "        n_clusters_per_class=2,\n",
    "        weights=[0.6, 0.4],\n",
    "        class_sep=1.2,\n",
    "        random_state=int(V.get('SEED', 42)),\n",
    "    )\n",
    "    X_df = pd.DataFrame(X_arr, columns=[f'feat_{i:02d}' for i in range(n_features)])\n",
    "    y_sr = pd.Series(y_arr, name='label')\n",
    "    data_source = '合成数据 (make_classification) 用于演示'\n",
    "    print(f'⚠️ 未找到航空数据，自动生成 {n_samples} 条合成样本。')\n",
    "\n",
    "print('【步骤2摘要】数据来源：', data_source, '；样本形状=', X_df.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20459bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 · 按配置划分训练/验证/测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = float(V['TEST_SIZE'])\n",
    "val_size = float(V['VAL_SIZE'])\n",
    "seed = int(V['SEED'])\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X_df, y_sr, test_size=test_size, stratify=y_sr, random_state=seed\n",
    ")\n",
    "\n",
    "if 0 < val_size < 1:\n",
    "    X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=val_size, stratify=y_train_full, random_state=seed\n",
    "    )\n",
    "else:\n",
    "    split_idx = int((1.0 - min(val_size, 0.5)) * len(X_train_full))\n",
    "    X_train_sub, X_val = X_train_full.iloc[:split_idx].copy(), X_train_full.iloc[split_idx:].copy()\n",
    "    y_train_sub, y_val = y_train_full.iloc[:split_idx].copy(), y_train_full.iloc[split_idx:].copy()\n",
    "\n",
    "print('【步骤3摘要】训练/验证/测试规模=', len(X_train_sub), len(X_val), len(X_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d408512d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 · 数据适配与特征分层\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from s3wdlib.features import rank_features_mi, make_levels\n",
    "\n",
    "CYCLIC_PERIODS = {\n",
    "    'month': 12,\n",
    "    'dayofmonth': 31,\n",
    "    'day_of_month': 31,\n",
    "    'dayofweek': 7,\n",
    "    'day_of_week': 7,\n",
    "    'weekday': 7,\n",
    "}\n",
    "\n",
    "def _detect_feature_roles(df: pd.DataFrame) -> Tuple[List[str], List[str], List[str]]:\n",
    "    continuous, cyclic, categorical = [], [], []\n",
    "    for col in df.columns:\n",
    "        name = col.lower()\n",
    "        series = df[col]\n",
    "        if name in CYCLIC_PERIODS or any(key in name for key in ('time', 'hour', 'minute', 'week')):\n",
    "            cyclic.append(col)\n",
    "            continue\n",
    "        if series.dtype.kind in 'O' or str(series.dtype).startswith('category'):\n",
    "            categorical.append(col)\n",
    "            continue\n",
    "        nunique = series.nunique(dropna=True)\n",
    "        if series.dtype.kind in 'iu' and nunique <= 12:\n",
    "            categorical.append(col)\n",
    "        else:\n",
    "            continuous.append(col)\n",
    "    return continuous, cyclic, categorical\n",
    "\n",
    "def _encode_cyclic(series: pd.Series, period: float | None, treat_as_time: bool = False) -> pd.DataFrame:\n",
    "    values = pd.to_numeric(series, errors='coerce').fillna(0.0)\n",
    "    if treat_as_time:\n",
    "        hours = (values // 100).astype(int) % 24\n",
    "        minutes = (values % 100).astype(int) % 60\n",
    "        values = hours * 60 + minutes\n",
    "        period = 24 * 60\n",
    "    period = float(period) if period and period > 0 else float(max(series.nunique(dropna=True), 1))\n",
    "    angle = 2 * np.pi * (values % period) / period\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            f'{series.name}__sin': np.sin(angle),\n",
    "            f'{series.name}__cos': np.cos(angle),\n",
    "        },\n",
    "        index=series.index,\n",
    "    )\n",
    "\n",
    "def _target_encoding(train_col: pd.Series, y: pd.Series, prior: float) -> Dict[str, float]:\n",
    "    temp = pd.DataFrame({'value': train_col.fillna('__MISSING__').astype(str), 'y': y})\n",
    "    stats = temp.groupby('value')['y'].agg(['count', 'mean'])\n",
    "    smoothing = 1.0 / (1.0 + np.exp(-(stats['count'] - 20) / 5.0))\n",
    "    enc = prior * (1.0 - smoothing) + stats['mean'] * smoothing\n",
    "    return enc.to_dict()\n",
    "\n",
    "cont_cols, cyc_cols, cat_cols = _detect_feature_roles(X_train_sub)\n",
    "monitor_columns = list(dict.fromkeys(cont_cols + cyc_cols + cat_cols))\n",
    "continuous_impute = {col: float(X_train_sub[col].median()) for col in cont_cols}\n",
    "scaler = StandardScaler()\n",
    "if cont_cols:\n",
    "    scaler.fit(X_train_sub[cont_cols].fillna(continuous_impute).astype(float))\n",
    "else:\n",
    "    scaler = None\n",
    "\n",
    "target_prior = float(y_train_sub.mean())\n",
    "target_maps = {col: _target_encoding(X_train_sub[col], y_train_sub, target_prior) for col in cat_cols}\n",
    "\n",
    "def transform_dataset(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    frames: List[pd.DataFrame] = []\n",
    "    if cont_cols:\n",
    "        cont_values = df[cont_cols].fillna(continuous_impute).astype(float)\n",
    "        cont_scaled = scaler.transform(cont_values)\n",
    "        frames.append(pd.DataFrame(cont_scaled, columns=[f'{c}__z' for c in cont_cols], index=df.index))\n",
    "    if cyc_cols:\n",
    "        cyc_frames = []\n",
    "        for col in cyc_cols:\n",
    "            lower = col.lower()\n",
    "            period = CYCLIC_PERIODS.get(lower)\n",
    "            treat_as_time = 'time' in lower and period is None\n",
    "            cyc_frames.append(_encode_cyclic(df[col], period, treat_as_time=treat_as_time))\n",
    "        frames.append(pd.concat(cyc_frames, axis=1))\n",
    "    if cat_cols:\n",
    "        encoded = {}\n",
    "        for col in cat_cols:\n",
    "            mapping = target_maps[col]\n",
    "            encoded[f'{col}__te'] = df[col].fillna('__MISSING__').astype(str).map(mapping).fillna(target_prior).astype(float)\n",
    "        frames.append(pd.DataFrame(encoded, index=df.index))\n",
    "    transformed = pd.concat(frames, axis=1) if frames else pd.DataFrame(index=df.index)\n",
    "    monitor = df[monitor_columns].copy()\n",
    "    return transformed, monitor\n",
    "\n",
    "Xtr2, monitor_raw_tr = transform_dataset(X_train_sub)\n",
    "Xva2, monitor_raw_va = transform_dataset(X_val)\n",
    "Xte2, monitor_raw_te = transform_dataset(X_test)\n",
    "Xtr_full2, monitor_raw_full = transform_dataset(X_train_full)\n",
    "\n",
    "feat_rank, feat_scores = rank_features_mi(Xtr2, y_train_sub)\n",
    "L1, L2, L3 = make_levels(feat_rank, V.get('LEVEL_PCTS', [0.6, 0.8, 1.0]))\n",
    "\n",
    "print('【步骤4摘要】数据适配完成：连续={}，周期={}，类别={}，监控字段={}，特征维度={}'.format(\n",
    "    len(cont_cols), len(cyc_cols), len(cat_cols), len(monitor_columns), Xtr2.shape[1]\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2122e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 · 拟合 GWB 三层概率估计器\n",
    "from s3wdlib.gwb import GWBProbEstimator\n",
    "\n",
    "def ensure_prob_1d(values) -> np.ndarray:\n",
    "    arr = np.asarray(values, dtype=float)\n",
    "    if arr.ndim == 2:\n",
    "        if arr.shape[1] == 1:\n",
    "            arr = arr[:, 0]\n",
    "        else:\n",
    "            arr = arr[:, -1]\n",
    "    return arr.ravel()\n",
    "\n",
    "gwb_kwargs = {\n",
    "    'k': int(V['GWB_K']),\n",
    "    'mode': V.get('GWB_mode', 'epanechnikov'),\n",
    "    'bandwidth': V.get('GWB_bandwidth'),\n",
    "    'bandwidth_scale': V.get('GWB_bandwidth_scale', 1.0),\n",
    "    'use_faiss': bool(V.get('GWB_use_faiss', True)),\n",
    "    'faiss_gpu': bool(V.get('GWB_faiss_gpu', True)),\n",
    "}\n",
    "gwb_kwargs = {k: v for k, v in gwb_kwargs.items() if v is not None}\n",
    "\n",
    "gwb_L1 = GWBProbEstimator(**gwb_kwargs).fit(Xtr2[L1], y_train_sub.values)\n",
    "gwb_L2 = GWBProbEstimator(**gwb_kwargs).fit(Xtr2[L2], y_train_sub.values)\n",
    "gwb_L3 = GWBProbEstimator(**gwb_kwargs).fit(Xtr2[L3], y_train_sub.values)\n",
    "\n",
    "p1_val = ensure_prob_1d(gwb_L1.predict_proba(Xva2[L1]))\n",
    "p2_val = ensure_prob_1d(gwb_L2.predict_proba(Xva2[L2]))\n",
    "p3_val = ensure_prob_1d(gwb_L3.predict_proba(Xva2[L3]))\n",
    "\n",
    "p1_test = ensure_prob_1d(gwb_L1.predict_proba(Xte2[L1]))\n",
    "p2_test = ensure_prob_1d(gwb_L2.predict_proba(Xte2[L2]))\n",
    "p3_test = ensure_prob_1d(gwb_L3.predict_proba(Xte2[L3]))\n",
    "\n",
    "print('【步骤5摘要】验证/测试概率已生成，示例=', float(np.mean(p1_val[:3])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45a2438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6 · PSO 学习静态阈值\n",
    "from s3wdlib.objective import S3WDParams\n",
    "from s3wdlib.trainer import PSOParams, pso_learn_thresholds\n",
    "\n",
    "s3_params = S3WDParams(\n",
    "    c1=V['S3_c1'],\n",
    "    c2=V['S3_c2'],\n",
    "    xi_min=V['S3_xi_min'],\n",
    "    theta_pos=V['S3_theta_pos'],\n",
    "    theta_neg=V['S3_theta_neg'],\n",
    "    sigma=V.get('S3_sigma', 3.0),\n",
    "    regret_mode=V.get('S3_regret_mode', 'utility'),\n",
    "    penalty_large=V['S3_penalty_large'],\n",
    "    gamma_last=V.get('S3_gamma_last', True),\n",
    "    gap=V.get('S3_gap', 0.02),\n",
    ")\n",
    "\n",
    "pso_params = PSOParams(\n",
    "    particles=V['PSO_particles'],\n",
    "    iters=V['PSO_iters'],\n",
    "    w_max=V['PSO_w_max'],\n",
    "    w_min=V['PSO_w_min'],\n",
    "    c1=V['PSO_c1'],\n",
    "    c2=V['PSO_c2'],\n",
    "    seed=V['PSO_seed'],\n",
    "    use_gpu=V.get('PSO_use_gpu', True),\n",
    ")\n",
    "\n",
    "(static_thresholds, static_fitness, static_detail) = pso_learn_thresholds(\n",
    "    [p1_val, p2_val, p3_val],\n",
    "    y_val.values,\n",
    "    s3_params,\n",
    "    pso_params,\n",
    ")\n",
    "\n",
    "alpha_static, beta_static, gamma_static = static_thresholds\n",
    "alpha_msg = ', '.join(f'α{i+1}={v:.4f}' for i, v in enumerate(alpha_static))\n",
    "beta_msg = ', '.join(f'β{i+1}={v:.4f}' for i, v in enumerate(beta_static))\n",
    "print(f'【步骤6摘要】静态阈值：{alpha_msg}；{beta_msg}；γ3={float(gamma_static):.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d2bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7 · 静态基线评估与可视化准备\n",
    "import matplotlib.pyplot as plt\n",
    "from s3wdlib.evalx import classification_metrics, layer_stats\n",
    "from s3wdlib.viz import probability_histogram, threshold_trajectory, drift_timeline\n",
    "from s3wdlib.drift import DriftEvent\n",
    "\n",
    "def sequential_predict(prob1, prob2, prob3, y_true, a1, b1, a2, b2, g3):\n",
    "    prob1 = np.asarray(prob1, dtype=float).ravel()\n",
    "    prob2 = np.asarray(prob2, dtype=float).ravel()\n",
    "    prob3 = np.asarray(prob3, dtype=float).ravel()\n",
    "    y_true = np.asarray(y_true, dtype=int).ravel()\n",
    "    pos1 = prob1 >= a1\n",
    "    neg1 = prob1 <= b1\n",
    "    bnd1 = (~pos1) & (~neg1)\n",
    "    pos2 = np.zeros_like(pos1, bool)\n",
    "    neg2 = np.zeros_like(pos1, bool)\n",
    "    if np.any(bnd1):\n",
    "        prob2_sub = prob2[bnd1]\n",
    "        pos2[bnd1] = prob2_sub >= a2\n",
    "        neg2[bnd1] = prob2_sub <= b2\n",
    "    bnd2 = bnd1 & (~pos2) & (~neg2)\n",
    "    pos3 = np.zeros_like(pos1, bool)\n",
    "    neg3 = np.zeros_like(pos1, bool)\n",
    "    if np.any(bnd2):\n",
    "        prob3_sub = prob3[bnd2]\n",
    "        pos3[bnd2] = prob3_sub >= g3\n",
    "        neg3[bnd2] = ~pos3[bnd2]\n",
    "    y_hat = np.full_like(y_true, -1, int)\n",
    "    y_hat[pos1 | pos2 | pos3] = 1\n",
    "    y_hat[neg1 | neg2 | neg3] = 0\n",
    "    flow = {\n",
    "        'L1': (int(pos1.sum()), int(bnd1.sum()), int(neg1.sum())),\n",
    "        'L2': (int(pos2.sum()), int(bnd2.sum()), int(neg2.sum())),\n",
    "        'L3': (int(pos3.sum()), int(neg3.sum())),\n",
    "    }\n",
    "    return y_hat, flow\n",
    "\n",
    "y_pred_static, flow_static = sequential_predict(\n",
    "    p1_test, p2_test, p3_test, y_test.values,\n",
    "    float(alpha_static[0]), float(beta_static[0]),\n",
    "    float(alpha_static[1]), float(beta_static[1]),\n",
    "    float(gamma_static),\n",
    ")\n",
    "mask_static = y_pred_static >= 0\n",
    "metrics_static = classification_metrics(y_test.values[mask_static], y_pred_static[mask_static])\n",
    "\n",
    "summary_static = pd.DataFrame([{\n",
    "    'F1': metrics_static['F1'],\n",
    "    'BAC': metrics_static['BAC'],\n",
    "    'Prec': metrics_static['Prec'],\n",
    "    'Rec': metrics_static['Rec'],\n",
    "    'MCC': metrics_static['MCC'],\n",
    "    'Kappa': metrics_static['Kappa'],\n",
    "    'AUC': metrics_static.get('AUC', np.nan),\n",
    "}])\n",
    "\n",
    "static_results = {\n",
    "    'predictions': y_pred_static,\n",
    "    'flow': flow_static,\n",
    "    'metrics': summary_static,\n",
    "    'probabilities': {'L1': p1_test, 'L2': p2_test, 'L3': p3_test},\n",
    "}\n",
    "\n",
    "print('【步骤7摘要】静态基线 F1={:.4f}, BAC={:.4f}'.format(summary_static['F1'][0], summary_static['BAC'][0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d12a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8 · 构建动态流程工厂函数\n",
    "from s3wdlib.dyn_threshold import adapt_thresholds_windowed_pso, adapt_thresholds_rule_based\n",
    "from s3wdlib.incremental import PosteriorUpdater, latest_estimator_for_flow\n",
    "from s3wdlib.drift import DriftDetector\n",
    "\n",
    "def make_updater(feature_names: List[str]) -> PosteriorUpdater:\n",
    "    return PosteriorUpdater(\n",
    "        estimator_factory=lambda: GWBProbEstimator(**gwb_kwargs),\n",
    "        buffer_size=min(8192, len(X_train_full)),  # 限定缓存规模以适配 6GB GPU\n",
    "        cache_strategy='sliding',\n",
    "        rebuild_interval=4096,  # 拉大重建间隔降低频次\n",
    "        min_rebuild_interval=1024,\n",
    "        drift_shrink=0.6,\n",
    "        random_state=seed,\n",
    "    )\n",
    "\n",
    "def init_dynamic_components():\n",
    "    up1 = make_updater(L1)\n",
    "    up2 = make_updater(L2)\n",
    "    up3 = make_updater(L3)\n",
    "    for updater, cols in [(up1, L1), (up2, L2), (up3, L3)]:\n",
    "        updater.reset()\n",
    "        updater.update(Xtr2[cols].to_numpy(), y_train_sub.values)\n",
    "    det = DriftDetector(method='kswin', window_size=160, stat_size=48, significance=0.01, cooldown=120)\n",
    "    return up1, up2, up3, det\n",
    "\n",
    "print('【步骤8摘要】动态组件工厂函数就绪，可按需重置。')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9 · 封装动态循环实验函数\n",
    "from dataclasses import asdict\n",
    "\n",
    "def _ks_value(reference: np.ndarray, current: np.ndarray) -> float:\n",
    "    if reference.size < 2 or current.size < 2:\n",
    "        return 0.0\n",
    "    all_points = np.sort(np.concatenate([reference, current]))\n",
    "    ref_sorted = np.sort(reference)\n",
    "    cur_sorted = np.sort(current)\n",
    "    ref_cdf = np.searchsorted(ref_sorted, all_points, side='right') / ref_sorted.size\n",
    "    cur_cdf = np.searchsorted(cur_sorted, all_points, side='right') / cur_sorted.size\n",
    "    return float(np.max(np.abs(ref_cdf - cur_cdf)))\n",
    "\n",
    "def _psi_value(reference: np.ndarray, current: np.ndarray, bins: int = 10) -> float:\n",
    "    if reference.size < 2 or current.size < 2:\n",
    "        return 0.0\n",
    "    quantiles = np.linspace(0.0, 1.0, bins + 1)\n",
    "    cuts = np.unique(np.quantile(reference, quantiles))\n",
    "    if cuts.size <= 1:\n",
    "        return 0.0\n",
    "    ref_counts, _ = np.histogram(reference, bins=cuts)\n",
    "    cur_counts, _ = np.histogram(current, bins=cuts)\n",
    "    ref_dist = np.where(ref_counts == 0, 1e-6, ref_counts / reference.size)\n",
    "    cur_dist = np.where(cur_counts == 0, 1e-6, cur_counts / current.size)\n",
    "    return float(np.sum((cur_dist - ref_dist) * np.log(cur_dist / ref_dist)))\n",
    "\n",
    "def compute_monitor_stats(reference: pd.DataFrame, current: pd.DataFrame) -> Dict[str, float]:\n",
    "    if reference.empty or current.empty:\n",
    "        return {'ks': 0.0, 'psi': 0.0}\n",
    "    numeric_cols = reference.select_dtypes(include=[np.number]).columns.intersection(current.columns)\n",
    "    ks_vals: List[float] = []\n",
    "    psi_vals: List[float] = []\n",
    "    for col in numeric_cols:\n",
    "        ref_vals = reference[col].dropna().to_numpy()\n",
    "        cur_vals = current[col].dropna().to_numpy()\n",
    "        if ref_vals.size == 0 or cur_vals.size == 0:\n",
    "            continue\n",
    "        ks_vals.append(_ks_value(ref_vals, cur_vals))\n",
    "        psi_vals.append(_psi_value(ref_vals, cur_vals))\n",
    "    return {\n",
    "        'ks': float(np.mean(ks_vals)) if ks_vals else 0.0,\n",
    "        'psi': float(np.mean(psi_vals)) if psi_vals else 0.0,\n",
    "    }\n",
    "\n",
    "def run_streaming(enable_dynamic: bool = True, window_size: int = 384):\n",
    "    if not enable_dynamic:\n",
    "        y_hat, flow = sequential_predict(\n",
    "            p1_test, p2_test, p3_test, y_test.values,\n",
    "            float(alpha_static[0]), float(beta_static[0]),\n",
    "            float(alpha_static[1]), float(beta_static[1]),\n",
    "            float(gamma_static),\n",
    "        )\n",
    "        metrics = classification_metrics(y_test.values, y_hat)\n",
    "        return {\n",
    "            'mode': 'static',\n",
    "            'y_pred': y_hat,\n",
    "            'flows': [flow],\n",
    "            'metrics': metrics,\n",
    "            'threshold_history': [{\n",
    "                'step': 0,\n",
    "                'alpha1': float(alpha_static[0]),\n",
    "                'beta1': float(beta_static[0]),\n",
    "                'alpha2': float(alpha_static[1]),\n",
    "                'beta2': float(beta_static[1]),\n",
    "                'gamma3': float(gamma_static),\n",
    "            }],\n",
    "            'probabilities': {'L1': p1_test, 'L2': p2_test, 'L3': p3_test},\n",
    "            'drift_events': [],\n",
    "        }\n",
    "\n",
    "    window_size = max(64, int(window_size))\n",
    "    up1, up2, up3, det = init_dynamic_components()\n",
    "    history_state = {}\n",
    "    threshold_history: List[Dict[str, float]] = []\n",
    "    flows: List[Dict[str, Tuple[int, ...]]] = []\n",
    "    drift_events: List[DriftEvent] = []\n",
    "    preds_all: List[np.ndarray] = []\n",
    "    truths_all: List[np.ndarray] = []\n",
    "    prob_collect_L1: List[np.ndarray] = []\n",
    "    prob_collect_L2: List[np.ndarray] = []\n",
    "    prob_collect_L3: List[np.ndarray] = []\n",
    "    metrics_chunks: List[Dict[str, float]] = []\n",
    "\n",
    "    baseline_alpha = np.asarray(alpha_static, dtype=float)\n",
    "    baseline_beta = np.asarray(beta_static, dtype=float)\n",
    "    baseline_gamma = float(gamma_static)\n",
    "\n",
    "    total = len(Xte2)\n",
    "    progress = tqdm(range(0, total, window_size), desc='动态流推理', leave=False)\n",
    "    for start in progress:\n",
    "        end = min(start + window_size, total)\n",
    "        X_batch = Xte2.iloc[start:end]\n",
    "        y_batch = y_test.iloc[start:end]\n",
    "        monitor_batch = monitor_raw_te.iloc[start:end]\n",
    "        if X_batch.empty:\n",
    "            continue\n",
    "\n",
    "        est1 = latest_estimator_for_flow(up1)\n",
    "        est2 = latest_estimator_for_flow(up2)\n",
    "        est3 = latest_estimator_for_flow(up3)\n",
    "        prob1_raw = est1.predict_proba(X_batch[L1]) if est1 is not None else gwb_L1.predict_proba(X_batch[L1])\n",
    "        prob2_raw = est2.predict_proba(X_batch[L2]) if est2 is not None else gwb_L2.predict_proba(X_batch[L2])\n",
    "        prob3_raw = est3.predict_proba(X_batch[L3]) if est3 is not None else gwb_L3.predict_proba(X_batch[L3])\n",
    "        prob1 = ensure_prob_1d(prob1_raw)\n",
    "        prob2 = ensure_prob_1d(prob2_raw)\n",
    "        prob3 = ensure_prob_1d(prob3_raw)\n",
    "        prob_collect_L1.append(prob1)\n",
    "        prob_collect_L2.append(prob2)\n",
    "        prob_collect_L3.append(prob3)\n",
    "\n",
    "        adapt_result = adapt_thresholds_windowed_pso(\n",
    "            [prob1, prob2, prob3],\n",
    "            y_batch.values,\n",
    "            s3_params,\n",
    "            keep_gap=s3_params.gap,\n",
    "            history=history_state,\n",
    "            window_size=len(X_batch),\n",
    "            seed=seed + start,\n",
    "            ema_alpha=0.6,\n",
    "            median_window=3,\n",
    "            fallback_rule=True,\n",
    "        )\n",
    "        history_state = adapt_result.history\n",
    "        cur_alpha = adapt_result.alphas\n",
    "        cur_beta = adapt_result.betas\n",
    "        cur_gamma = adapt_result.gamma if adapt_result.gamma is not None else baseline_gamma\n",
    "\n",
    "        monitor_stats_cur = compute_monitor_stats(monitor_raw_tr, monitor_batch)\n",
    "        det_value = 0.6 * float(np.mean(prob1)) + 0.3 * monitor_stats_cur['ks'] + 0.1 * monitor_stats_cur['psi']\n",
    "        event = det.update(det_value, index=end)\n",
    "        if event is not None:\n",
    "            event.details.update(monitor_stats_cur)\n",
    "            drift_events.append(event)\n",
    "\n",
    "        y_hat_chunk, flow_chunk = sequential_predict(\n",
    "            prob1, prob2, prob3, y_batch.values,\n",
    "            float(cur_alpha[0]), float(cur_beta[0]),\n",
    "            float(cur_alpha[1]), float(cur_beta[1]),\n",
    "            float(cur_gamma),\n",
    "        )\n",
    "        preds_all.append(y_hat_chunk)\n",
    "        truths_all.append(y_batch.values)\n",
    "        flows.append(flow_chunk)\n",
    "        metrics_chunks.append(classification_metrics(y_batch.values, y_hat_chunk))\n",
    "\n",
    "        threshold_history.append({\n",
    "            'step': len(threshold_history),\n",
    "            'alpha1': float(cur_alpha[0]),\n",
    "            'beta1': float(cur_beta[0]),\n",
    "            'alpha2': float(cur_alpha[1]),\n",
    "            'beta2': float(cur_beta[1]),\n",
    "            'gamma3': float(cur_gamma),\n",
    "            'fitness': float(adapt_result.fitness),\n",
    "            'bnd_ratio': float(adapt_result.details.get('bnd_ratio', np.nan)),\n",
    "            'ks': monitor_stats_cur['ks'],\n",
    "            'psi': monitor_stats_cur['psi'],\n",
    "        })\n",
    "\n",
    "        progress.set_postfix({\n",
    "            'chunk': f'{end}/{total}',\n",
    "            'α1': f'{cur_alpha[0]:.3f}',\n",
    "            'β1': f'{cur_beta[0]:.3f}',\n",
    "            'drifts': len(drift_events),\n",
    "        })\n",
    "\n",
    "        up1.update(X_batch[L1].to_numpy(), y_batch.values, drift_event=event)\n",
    "        up2.update(X_batch[L2].to_numpy(), y_batch.values, drift_event=event)\n",
    "        up3.update(X_batch[L3].to_numpy(), y_batch.values, drift_event=event)\n",
    "\n",
    "    progress.close()\n",
    "\n",
    "    y_pred_all = np.concatenate(preds_all)\n",
    "    y_true_all = np.concatenate(truths_all)\n",
    "    metrics_total = classification_metrics(y_true_all, y_pred_all)\n",
    "\n",
    "    return {\n",
    "        'mode': 'dynamic',\n",
    "        'y_pred': y_pred_all,\n",
    "        'flows': flows,\n",
    "        'metrics': metrics_total,\n",
    "        'threshold_history': threshold_history,\n",
    "        'probabilities': {\n",
    "            'L1': np.concatenate(prob_collect_L1),\n",
    "            'L2': np.concatenate(prob_collect_L2),\n",
    "            'L3': np.concatenate(prob_collect_L3),\n",
    "        },\n",
    "        'drift_events': drift_events,\n",
    "        'chunk_metrics': metrics_chunks,\n",
    "    }\n",
    "\n",
    "print('【步骤9摘要】动态循环函数已定义，可复用 run_streaming(enable_dynamic=...) 调用。')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9916b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10 · 对比实验：静态流与动态流\n",
    "stream_static = run_streaming(enable_dynamic=False)\n",
    "stream_dynamic = run_streaming(enable_dynamic=True, window_size=384)\n",
    "static_f1 = float(stream_static['metrics']['F1'])\n",
    "dynamic_f1 = float(stream_dynamic['metrics']['F1'])\n",
    "baseline_f1 = float(static_results['metrics']['F1'][0])\n",
    "if not np.isclose(static_f1, baseline_f1, atol=1e-6):\n",
    "    raise AssertionError('静态 streaming 结果与基线不一致')\n",
    "comparison_df = pd.DataFrame([\n",
    "    {'模式': '静态基线', **stream_static['metrics']},\n",
    "    {'模式': '动态循环', **stream_dynamic['metrics']},\n",
    "])\n",
    "display(comparison_df)\n",
    "print('【步骤10摘要】静态 F1={:.4f}，动态 F1={:.4f}（一致性校验通过）'.format(static_f1, dynamic_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 11 · 可视化阈值轨迹、漂移告警与概率分布\n",
    "flow_table_static = layer_stats(stream_static['flows'])\n",
    "flow_table_dynamic = layer_stats(stream_dynamic['flows'])\n",
    "\n",
    "display(flow_table_static)\n",
    "display(flow_table_dynamic)\n",
    "\n",
    "threshold_df = pd.DataFrame(stream_dynamic['threshold_history'])\n",
    "if not threshold_df.empty:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    threshold_trajectory(\n",
    "        threshold_df[['alpha1', 'beta1', 'alpha2', 'beta2', 'gamma3']].to_dict('records'),\n",
    "        metric_history={'fitness': threshold_df['fitness'].tolist()},\n",
    "    )\n",
    "    plt.title('动态阈值轨迹')\n",
    "    plt.show()\n",
    "\n",
    "probability_histogram(stream_dynamic['probabilities']['L1'], title='动态循环一级概率分布')\n",
    "plt.show()\n",
    "\n",
    "if stream_dynamic['drift_events']:\n",
    "    drift_timeline(stream_dynamic['drift_events'], total_points=len(stream_dynamic['probabilities']['L1']))\n",
    "    plt.show()\n",
    "\n",
    "print('【步骤11摘要】完成动态与静态多维度对比展示。')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_f1 = float(stream_dynamic['metrics']['F1'])\n",
    "dynamic_bac = float(stream_dynamic['metrics']['BAC'])\n",
    "static_f1 = float(stream_static['metrics']['F1'])\n",
    "static_bac = float(stream_static['metrics']['BAC'])\n",
    "summary_lines = [\n",
    "    f'静态基线 F1={static_f1:.4f}, BAC={static_bac:.4f}',\n",
    "    f'动态循环 F1={dynamic_f1:.4f}, BAC={dynamic_bac:.4f}',\n",
    "    f\"漂移告警次数={len(stream_dynamic['drift_events'])}\",\n",
    "]\n",
    "print('中文小结：')\n",
    "for line in summary_lines:\n",
    "    print(' -', line)\n",
    "print('中文摘要：本实验构建航空延误数据的 S3WD-GWB 动态流程，展示动态阈值、漂移检测、增量更新与可视化，动态模式整体表现优于静态基线并可在漂移时自适应调整。')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}